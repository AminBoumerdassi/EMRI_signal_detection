
The following modules were not unloaded:
   (Use "module --force purge" to unload all):

  1) XALT/minimal   2) slurm   3) NeSI
2024-04-15 04:01:39.841760: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-15 04:01:39.841846: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-15 04:01:39.841879: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-15 04:01:39.852222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-15 04:01:48.537480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38298 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:05:00.0, compute capability: 8.0
1 Physical GPUs, 1 Logical GPUs
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d (Conv1D)             (None, 1048576, 32)       8224      
                                                                 
 conv1d_1 (Conv1D)           (None, 262144, 16)        65552     
                                                                 
 conv1d_2 (Conv1D)           (None, 65536, 8)          16392     
                                                                 
 conv1d_transpose (Conv1DTr  (None, 262144, 8)         8200      
 anspose)                                                        
                                                                 
 conv1d_transpose_1 (Conv1D  (None, 1048576, 16)       16400     
 Transpose)                                                      
                                                                 
 conv1d_transpose_2 (Conv1D  (None, 4194304, 32)       65568     
 Transpose)                                                      
                                                                 
 conv1d_transpose_3 (Conv1D  (None, 4194304, 2)        66        
 Transpose)                                                      
                                                                 
 activation (Activation)     (None, 4194304, 2)        0         
                                                                 
=================================================================
Total params: 180402 (704.70 KB)
Trainable params: 180402 (704.70 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
#################################
####DATA GENERATOR PARAMETERS####
#Dataset size:  11011
#Batch size:  8
#Time in years: 1.3290715810104465
#n_channels:  2
#dt:  10
#Length of timeseries: 4194304
Noise background:  False
#################################
Epoch 1/40
2024-04-15 04:02:07.277686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600
2024-04-15 04:04:49.868979: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ab0586759c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-15 04:04:49.869052: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2024-04-15 04:04:49.876516: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-04-15 04:04:50.183581: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
10/10 - 202s - loss: 2.9554e-05 - val_loss: 2.9555e-05 - 202s/epoch - 20s/step
Epoch 2/40
10/10 - 36s - loss: 2.9670e-05 - val_loss: 2.9670e-05 - 36s/epoch - 4s/step
Epoch 3/40
10/10 - 36s - loss: 3.1358e-05 - val_loss: 3.1359e-05 - 36s/epoch - 4s/step
Epoch 4/40
10/10 - 35s - loss: 1.4171e-05 - val_loss: 1.4170e-05 - 35s/epoch - 4s/step
Epoch 5/40
10/10 - 36s - loss: 7.9341e-05 - val_loss: 7.9341e-05 - 36s/epoch - 4s/step
Epoch 6/40
10/10 - 36s - loss: 5.6754e-05 - val_loss: 5.6749e-05 - 36s/epoch - 4s/step
Epoch 7/40
10/10 - 35s - loss: 2.4152e-05 - val_loss: 2.4153e-05 - 35s/epoch - 4s/step
Epoch 8/40
10/10 - 35s - loss: 5.1199e-05 - val_loss: 5.1197e-05 - 35s/epoch - 4s/step
Epoch 9/40
10/10 - 36s - loss: 8.6149e-05 - val_loss: 8.6143e-05 - 36s/epoch - 4s/step
Epoch 10/40
10/10 - 35s - loss: 6.6787e-05 - val_loss: 6.6249e-05 - 35s/epoch - 4s/step
Epoch 11/40
10/10 - 34s - loss: 2.3591e-05 - val_loss: 2.3597e-05 - 34s/epoch - 3s/step
Epoch 12/40
10/10 - 36s - loss: 7.8085e-05 - val_loss: 7.8080e-05 - 36s/epoch - 4s/step
Epoch 13/40
10/10 - 35s - loss: 2.8105e-05 - val_loss: 2.8114e-05 - 35s/epoch - 3s/step
Epoch 14/40
10/10 - 36s - loss: 4.6854e-05 - val_loss: 4.6850e-05 - 36s/epoch - 4s/step
Epoch 15/40
10/10 - 36s - loss: 4.3231e-05 - val_loss: 4.3229e-05 - 36s/epoch - 4s/step
Epoch 16/40
10/10 - 36s - loss: 4.7077e-05 - val_loss: 4.7073e-05 - 36s/epoch - 4s/step
Epoch 17/40
10/10 - 34s - loss: 6.3492e-05 - val_loss: 6.3491e-05 - 34s/epoch - 3s/step
Epoch 18/40
10/10 - 35s - loss: 6.3649e-05 - val_loss: 6.3649e-05 - 35s/epoch - 4s/step
Epoch 19/40
10/10 - 35s - loss: 3.0668e-05 - val_loss: 3.0688e-05 - 35s/epoch - 4s/step
Epoch 20/40
10/10 - 36s - loss: 5.2996e-05 - val_loss: 5.2998e-05 - 36s/epoch - 4s/step
Epoch 21/40
10/10 - 35s - loss: 2.0889e-05 - val_loss: 2.0888e-05 - 35s/epoch - 3s/step
Epoch 22/40
10/10 - 35s - loss: 4.5676e-05 - val_loss: 4.5668e-05 - 35s/epoch - 3s/step
Epoch 23/40
10/10 - 35s - loss: 6.5292e-05 - val_loss: 6.5291e-05 - 35s/epoch - 4s/step
Epoch 24/40
10/10 - 35s - loss: 4.6402e-05 - val_loss: 4.6477e-05 - 35s/epoch - 4s/step
Epoch 25/40
10/10 - 35s - loss: 3.8313e-05 - val_loss: 3.8314e-05 - 35s/epoch - 3s/step
Epoch 26/40
10/10 - 35s - loss: 5.6909e-05 - val_loss: 5.6906e-05 - 35s/epoch - 3s/step
Epoch 27/40
10/10 - 35s - loss: 4.3308e-05 - val_loss: 4.3256e-05 - 35s/epoch - 4s/step
Epoch 28/40
10/10 - 36s - loss: 3.8731e-05 - val_loss: 3.8732e-05 - 36s/epoch - 4s/step
Epoch 29/40
10/10 - 36s - loss: 4.5003e-05 - val_loss: 4.5004e-05 - 36s/epoch - 4s/step
Epoch 30/40
10/10 - 35s - loss: 7.3734e-05 - val_loss: 7.3735e-05 - 35s/epoch - 4s/step
Epoch 31/40
10/10 - 35s - loss: 8.4414e-05 - val_loss: 8.4109e-05 - 35s/epoch - 4s/step
Epoch 32/40
10/10 - 35s - loss: 2.3067e-05 - val_loss: 2.3048e-05 - 35s/epoch - 4s/step
Epoch 33/40
10/10 - 34s - loss: 2.3563e-05 - val_loss: 2.3563e-05 - 34s/epoch - 3s/step
Epoch 34/40
10/10 - 35s - loss: 5.3832e-05 - val_loss: 5.3832e-05 - 35s/epoch - 4s/step
Epoch 35/40
10/10 - 36s - loss: 7.7389e-05 - val_loss: 7.7380e-05 - 36s/epoch - 4s/step
Epoch 36/40
10/10 - 34s - loss: 3.0300e-05 - val_loss: 3.0301e-05 - 34s/epoch - 3s/step
Epoch 37/40
10/10 - 36s - loss: 3.9948e-05 - val_loss: 3.9950e-05 - 36s/epoch - 4s/step
Epoch 38/40
10/10 - 35s - loss: 3.2121e-05 - val_loss: 3.2105e-05 - 35s/epoch - 4s/step
Epoch 39/40
10/10 - 34s - loss: 3.2073e-05 - val_loss: 3.2055e-05 - 34s/epoch - 3s/step
Epoch 40/40
10/10 - 34s - loss: 5.2072e-05 - val_loss: 5.2073e-05 - 34s/epoch - 3s/step
