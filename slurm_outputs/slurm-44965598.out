
The following modules were not unloaded:
   (Use "module --force purge" to unload all):

  1) XALT/minimal   2) slurm   3) NeSI
2024-03-25 16:07:32.538012: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-25 16:07:32.538088: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-25 16:07:32.612049: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-25 16:07:33.097443: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-25 16:08:15.586066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38298 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:83:00.0, compute capability: 8.0
1 Physical GPUs, 1 Logical GPUs
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d (Conv1D)             (None, 1048576, 32)       8224      
                                                                 
 conv1d_1 (Conv1D)           (None, 262144, 16)        65552     
                                                                 
 conv1d_2 (Conv1D)           (None, 65536, 8)          16392     
                                                                 
 conv1d_transpose (Conv1DTr  (None, 262144, 8)         8200      
 anspose)                                                        
                                                                 
 conv1d_transpose_1 (Conv1D  (None, 1048576, 16)       16400     
 Transpose)                                                      
                                                                 
 conv1d_transpose_2 (Conv1D  (None, 4194304, 32)       65568     
 Transpose)                                                      
                                                                 
 conv1d_transpose_3 (Conv1D  (None, 4194304, 2)        66        
 Transpose)                                                      
                                                                 
 activation (Activation)     (None, 4194304, 2)        0         
                                                                 
=================================================================
Total params: 180402 (704.70 KB)
Trainable params: 180402 (704.70 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
#################################
####DATA GENERATOR PARAMETERS####
#Batch size:  8
#Time in years: 1.3290715810104465
#n_channels:  2
#dt:  10
#Length of timeseries: 4194304
Noise background:  False
#################################
Epoch 1/40
2024-03-25 16:08:51.306647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600
2024-03-25 16:11:39.171101: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ab2655fe310 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-03-25 16:11:39.171875: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2024-03-25 16:11:39.180996: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-03-25 16:11:39.650666: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
10/10 - 225s - loss: 1.3405e-05 - val_loss: 1.3292e-05 - 225s/epoch - 22s/step
Epoch 2/40
10/10 - 53s - loss: 3.7513e-05 - val_loss: 3.6997e-05 - 53s/epoch - 5s/step
Epoch 3/40
10/10 - 51s - loss: 1.0218e-05 - val_loss: 1.0091e-05 - 51s/epoch - 5s/step
Epoch 4/40
10/10 - 49s - loss: 1.9616e-05 - val_loss: 1.9618e-05 - 49s/epoch - 5s/step
Epoch 5/40
10/10 - 50s - loss: 8.7678e-06 - val_loss: 8.7625e-06 - 50s/epoch - 5s/step
Epoch 6/40
10/10 - 54s - loss: 2.8460e-05 - val_loss: 2.8451e-05 - 54s/epoch - 5s/step
Epoch 7/40
10/10 - 54s - loss: 1.8992e-05 - val_loss: 1.8983e-05 - 54s/epoch - 5s/step
Epoch 8/40
10/10 - 50s - loss: 7.1846e-06 - val_loss: 7.1812e-06 - 50s/epoch - 5s/step
Epoch 9/40
10/10 - 46s - loss: 8.2071e-06 - val_loss: 8.2035e-06 - 46s/epoch - 5s/step
Epoch 10/40
10/10 - 46s - loss: 1.5498e-05 - val_loss: 1.5493e-05 - 46s/epoch - 5s/step
Epoch 11/40
10/10 - 46s - loss: 1.1105e-05 - val_loss: 1.1102e-05 - 46s/epoch - 5s/step
Epoch 12/40
10/10 - 46s - loss: 1.0673e-05 - val_loss: 1.0669e-05 - 46s/epoch - 5s/step
Epoch 13/40
10/10 - 47s - loss: 3.1074e-05 - val_loss: 3.1108e-05 - 47s/epoch - 5s/step
Epoch 14/40
10/10 - 46s - loss: 5.0650e-05 - val_loss: 5.0602e-05 - 46s/epoch - 5s/step
Epoch 15/40
10/10 - 46s - loss: 1.2048e-05 - val_loss: 1.2022e-05 - 46s/epoch - 5s/step
Epoch 16/40
10/10 - 45s - loss: 3.9150e-05 - val_loss: 3.9123e-05 - 45s/epoch - 5s/step
Epoch 17/40
10/10 - 45s - loss: 1.0397e-05 - val_loss: 1.0388e-05 - 45s/epoch - 4s/step
Epoch 18/40
10/10 - 45s - loss: 9.6865e-06 - val_loss: 9.6787e-06 - 45s/epoch - 4s/step
Epoch 19/40
10/10 - 45s - loss: 8.3197e-06 - val_loss: 8.3064e-06 - 45s/epoch - 5s/step
Epoch 20/40
10/10 - 46s - loss: 1.0513e-05 - val_loss: 1.0490e-05 - 46s/epoch - 5s/step
Epoch 21/40
10/10 - 45s - loss: 5.9227e-06 - val_loss: 5.9010e-06 - 45s/epoch - 5s/step
Epoch 22/40
10/10 - 45s - loss: 3.3761e-05 - val_loss: 3.3716e-05 - 45s/epoch - 5s/step
Epoch 23/40
10/10 - 45s - loss: 1.4043e-05 - val_loss: 1.3954e-05 - 45s/epoch - 5s/step
Epoch 24/40
10/10 - 45s - loss: 9.4844e-06 - val_loss: 9.3920e-06 - 45s/epoch - 5s/step
Epoch 25/40
10/10 - 46s - loss: 7.8296e-06 - val_loss: 7.7084e-06 - 46s/epoch - 5s/step
Epoch 26/40
10/10 - 44s - loss: 8.1206e-06 - val_loss: 8.0585e-06 - 44s/epoch - 4s/step
Epoch 27/40
10/10 - 46s - loss: 3.1268e-05 - val_loss: 3.0992e-05 - 46s/epoch - 5s/step
Epoch 28/40
10/10 - 44s - loss: 3.0861e-06 - val_loss: 2.9938e-06 - 44s/epoch - 4s/step
Epoch 29/40
10/10 - 45s - loss: 8.8265e-06 - val_loss: 8.7273e-06 - 45s/epoch - 4s/step
Epoch 30/40
10/10 - 47s - loss: 2.4952e-05 - val_loss: 2.4844e-05 - 47s/epoch - 5s/step
Epoch 31/40
10/10 - 46s - loss: 1.7497e-05 - val_loss: 1.7371e-05 - 46s/epoch - 5s/step
Epoch 32/40
10/10 - 47s - loss: 3.0709e-05 - val_loss: 3.0484e-05 - 47s/epoch - 5s/step
Epoch 33/40
10/10 - 47s - loss: 1.0250e-05 - val_loss: 1.0011e-05 - 47s/epoch - 5s/step
Epoch 34/40
10/10 - 45s - loss: 8.5595e-06 - val_loss: 8.1723e-06 - 45s/epoch - 5s/step
Epoch 35/40
10/10 - 46s - loss: 5.3518e-06 - val_loss: 5.2516e-06 - 46s/epoch - 5s/step
Epoch 36/40
10/10 - 45s - loss: 2.6386e-05 - val_loss: 2.6181e-05 - 45s/epoch - 4s/step
Epoch 37/40
10/10 - 45s - loss: 3.7533e-06 - val_loss: 3.6684e-06 - 45s/epoch - 5s/step
Epoch 38/40
10/10 - 45s - loss: 2.7856e-06 - val_loss: 2.7467e-06 - 45s/epoch - 5s/step
Epoch 39/40
10/10 - 47s - loss: 5.0504e-06 - val_loss: 4.8817e-06 - 47s/epoch - 5s/step
Epoch 40/40
10/10 - 46s - loss: 7.0598e-06 - val_loss: 6.8426e-06 - 46s/epoch - 5s/step
