
The following modules were not unloaded:
   (Use "module --force purge" to unload all):

  1) XALT/minimal   2) slurm   3) NeSI
2024-04-29 19:59:09.213092: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-29 19:59:09.213154: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-29 19:59:09.299178: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-29 19:59:09.876274: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-29 20:00:03.829638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38298 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:05:00.0, compute capability: 8.0
1 Physical GPUs, 1 Logical GPUs
#################################
####DATA GENERATOR PARAMETERS####
#Dataset size:  11011
#Batch size:  8
#Time in years: 2.658143162020893
#n_channels:  2
#dt:  10
#Length of timeseries: 8388608
Noise background:  False
#################################
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d (Conv1D)             (None, 1048576, 64)       8256      
                                                                 
 conv1d_1 (Conv1D)           (None, 131072, 64)        262208    
                                                                 
 conv1d_2 (Conv1D)           (None, 16384, 64)         262208    
                                                                 
 conv1d_transpose (Conv1DTr  (None, 131072, 64)        262208    
 anspose)                                                        
                                                                 
 conv1d_transpose_1 (Conv1D  (None, 1048576, 64)       262208    
 Transpose)                                                      
                                                                 
 conv1d_transpose_2 (Conv1D  (None, 8388608, 2)        8194      
 Transpose)                                                      
                                                                 
 activation (Activation)     (None, 8388608, 2)        0         
                                                                 
=================================================================
Total params: 1065282 (4.06 MB)
Trainable params: 1065282 (4.06 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
Epoch 1/100
2024-04-29 20:00:41.734916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600
2024-04-29 20:02:07.458625: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ab5dae030e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-29 20:02:07.458679: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2024-04-29 20:02:07.472558: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-04-29 20:02:07.472598: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-04-29 20:02:08.015541: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
10/10 - 158s - loss: 3.0148 - val_loss: 3.0102 - 158s/epoch - 16s/step
Epoch 2/100
10/10 - 69s - loss: 2.6148 - val_loss: 2.6134 - 69s/epoch - 7s/step
Epoch 3/100
10/10 - 70s - loss: 2.3023 - val_loss: 2.3007 - 70s/epoch - 7s/step
Epoch 4/100
10/10 - 70s - loss: 1.5834 - val_loss: 1.5851 - 70s/epoch - 7s/step
Epoch 5/100
10/10 - 70s - loss: 4.6593 - val_loss: 4.6595 - 70s/epoch - 7s/step
Epoch 6/100
10/10 - 71s - loss: 4.0603 - val_loss: 4.0692 - 71s/epoch - 7s/step
Epoch 7/100
10/10 - 70s - loss: 1.7232 - val_loss: 1.7185 - 70s/epoch - 7s/step
Epoch 8/100
10/10 - 70s - loss: 3.7431 - val_loss: 3.7282 - 70s/epoch - 7s/step
Epoch 9/100
10/10 - 70s - loss: 4.8645 - val_loss: 4.6282 - 70s/epoch - 7s/step
Epoch 10/100
10/10 - 70s - loss: 3.8738 - val_loss: 3.5240 - 70s/epoch - 7s/step
Epoch 11/100
10/10 - 69s - loss: 2.2839 - val_loss: 2.2448 - 69s/epoch - 7s/step
Epoch 12/100
10/10 - 71s - loss: 4.1488 - val_loss: 4.1172 - 71s/epoch - 7s/step
Epoch 13/100
10/10 - 69s - loss: 2.0274 - val_loss: 2.0130 - 69s/epoch - 7s/step
Epoch 14/100
10/10 - 71s - loss: 2.6769 - val_loss: 2.6575 - 71s/epoch - 7s/step
Epoch 15/100
10/10 - 70s - loss: 3.3879 - val_loss: 3.3798 - 70s/epoch - 7s/step
Epoch 16/100
10/10 - 70s - loss: 3.1753 - val_loss: 3.1579 - 70s/epoch - 7s/step
Epoch 17/100
10/10 - 68s - loss: 3.2596 - val_loss: 3.2535 - 68s/epoch - 7s/step
Epoch 18/100
10/10 - 70s - loss: 4.0131 - val_loss: 3.7278 - 70s/epoch - 7s/step
Epoch 19/100
10/10 - 69s - loss: 2.3569 - val_loss: 2.3313 - 69s/epoch - 7s/step
Epoch 20/100
10/10 - 71s - loss: 2.3487 - val_loss: 2.3147 - 71s/epoch - 7s/step
Epoch 21/100
10/10 - 68s - loss: 1.7114 - val_loss: 1.7020 - 68s/epoch - 7s/step
Epoch 22/100
10/10 - 68s - loss: 2.6775 - val_loss: 2.6622 - 68s/epoch - 7s/step
Epoch 23/100
10/10 - 68s - loss: 2.6953 - val_loss: 2.6819 - 68s/epoch - 7s/step
Epoch 24/100
10/10 - 69s - loss: 2.6323 - val_loss: 2.5933 - 69s/epoch - 7s/step
Epoch 25/100
10/10 - 67s - loss: 2.5201 - val_loss: 2.5131 - 67s/epoch - 7s/step
Epoch 26/100
10/10 - 68s - loss: 3.3828 - val_loss: 3.2832 - 68s/epoch - 7s/step
Epoch 27/100
10/10 - 69s - loss: 2.9669 - val_loss: 2.9168 - 69s/epoch - 7s/step
Epoch 28/100
10/10 - 70s - loss: 2.1861 - val_loss: 2.1717 - 70s/epoch - 7s/step
Epoch 29/100
10/10 - 70s - loss: 2.9045 - val_loss: 2.8924 - 70s/epoch - 7s/step
Epoch 30/100
10/10 - 68s - loss: 2.9230 - val_loss: 2.9215 - 68s/epoch - 7s/step
Epoch 31/100
10/10 - 69s - loss: 3.3441 - val_loss: 2.9875 - 69s/epoch - 7s/step
Epoch 32/100
10/10 - 69s - loss: 2.3465 - val_loss: 2.4158 - 69s/epoch - 7s/step
Epoch 33/100
10/10 - 68s - loss: 2.4493 - val_loss: 2.4017 - 68s/epoch - 7s/step
Epoch 34/100
10/10 - 67s - loss: 2.4009 - val_loss: 2.3861 - 67s/epoch - 7s/step
Epoch 35/100
10/10 - 69s - loss: 2.7609 - val_loss: 3.1452 - 69s/epoch - 7s/step
Epoch 36/100
10/10 - 67s - loss: 2.4611 - val_loss: 2.4508 - 67s/epoch - 7s/step
Epoch 37/100
10/10 - 69s - loss: 2.2174 - val_loss: 2.2166 - 69s/epoch - 7s/step
Epoch 38/100
10/10 - 69s - loss: 1.9052 - val_loss: 1.8942 - 69s/epoch - 7s/step
Epoch 39/100
10/10 - 66s - loss: 3.3459 - val_loss: 3.3347 - 66s/epoch - 7s/step
Epoch 40/100
10/10 - 66s - loss: 3.0764 - val_loss: 3.1054 - 66s/epoch - 7s/step
Epoch 41/100
10/10 - 70s - loss: 3.4197 - val_loss: 3.3843 - 70s/epoch - 7s/step
Epoch 42/100
10/10 - 68s - loss: 2.5768 - val_loss: 2.5939 - 68s/epoch - 7s/step
Epoch 43/100
10/10 - 67s - loss: 1.7878 - val_loss: 1.8284 - 67s/epoch - 7s/step
Epoch 44/100
10/10 - 70s - loss: 2.4529 - val_loss: 2.3900 - 70s/epoch - 7s/step
Epoch 45/100
10/10 - 69s - loss: 2.4775 - val_loss: 2.4469 - 69s/epoch - 7s/step
Epoch 46/100
10/10 - 68s - loss: 1.3913 - val_loss: 1.3723 - 68s/epoch - 7s/step
Epoch 47/100
10/10 - 68s - loss: 2.7819 - val_loss: 2.7653 - 68s/epoch - 7s/step
Epoch 48/100
10/10 - 70s - loss: 2.6123 - val_loss: 2.6095 - 70s/epoch - 7s/step
Epoch 49/100
10/10 - 69s - loss: 1.9329 - val_loss: 1.9280 - 69s/epoch - 7s/step
Epoch 50/100
10/10 - 68s - loss: 2.3127 - val_loss: 2.3070 - 68s/epoch - 7s/step
Epoch 51/100
10/10 - 70s - loss: 2.8749 - val_loss: 2.8712 - 70s/epoch - 7s/step
Epoch 52/100
10/10 - 71s - loss: 2.0878 - val_loss: 2.0816 - 71s/epoch - 7s/step
Epoch 53/100
10/10 - 67s - loss: 2.2739 - val_loss: 2.2881 - 67s/epoch - 7s/step
Epoch 54/100
10/10 - 67s - loss: 2.3432 - val_loss: 2.4530 - 67s/epoch - 7s/step
Epoch 55/100
10/10 - 70s - loss: 1.4952 - val_loss: 1.4919 - 70s/epoch - 7s/step
Epoch 56/100
10/10 - 75s - loss: 1.6568 - val_loss: 1.6460 - 75s/epoch - 8s/step
Epoch 57/100
10/10 - 70s - loss: 2.6348 - val_loss: 2.6403 - 70s/epoch - 7s/step
Epoch 58/100
10/10 - 70s - loss: 2.6794 - val_loss: 2.6839 - 70s/epoch - 7s/step
Epoch 59/100
10/10 - 70s - loss: 2.9059 - val_loss: 2.9040 - 70s/epoch - 7s/step
Epoch 60/100
10/10 - 69s - loss: 1.6082 - val_loss: 1.6434 - 69s/epoch - 7s/step
Epoch 61/100
10/10 - 69s - loss: 2.3537 - val_loss: 2.3494 - 69s/epoch - 7s/step
Epoch 62/100
10/10 - 70s - loss: 2.8882 - val_loss: 2.8945 - 70s/epoch - 7s/step
Epoch 63/100
10/10 - 72s - loss: 2.7823 - val_loss: 2.6315 - 72s/epoch - 7s/step
Epoch 64/100
10/10 - 70s - loss: 3.4037 - val_loss: 3.3397 - 70s/epoch - 7s/step
Epoch 65/100
10/10 - 68s - loss: 2.0687 - val_loss: 2.0615 - 68s/epoch - 7s/step
Epoch 66/100
10/10 - 69s - loss: 3.4532 - val_loss: 3.4675 - 69s/epoch - 7s/step
Epoch 67/100
10/10 - 68s - loss: 2.5469 - val_loss: 2.5464 - 68s/epoch - 7s/step
Epoch 68/100
10/10 - 70s - loss: 1.6945 - val_loss: 1.6904 - 70s/epoch - 7s/step
Epoch 69/100
10/10 - 70s - loss: 2.5343 - val_loss: 2.5461 - 70s/epoch - 7s/step
Epoch 70/100
10/10 - 69s - loss: 2.2672 - val_loss: 2.2595 - 69s/epoch - 7s/step
Epoch 71/100
10/10 - 71s - loss: 1.8192 - val_loss: 1.9098 - 71s/epoch - 7s/step
Epoch 72/100
10/10 - 70s - loss: 1.6333 - val_loss: 1.5574 - 70s/epoch - 7s/step
Epoch 73/100
10/10 - 69s - loss: 1.7635 - val_loss: 1.7460 - 69s/epoch - 7s/step
Epoch 74/100
10/10 - 72s - loss: 2.8665 - val_loss: 2.8316 - 72s/epoch - 7s/step
Epoch 75/100
10/10 - 69s - loss: 4.6492 - val_loss: 4.5462 - 69s/epoch - 7s/step
Epoch 76/100
10/10 - 69s - loss: 2.5057 - val_loss: 2.5041 - 69s/epoch - 7s/step
Epoch 77/100
10/10 - 70s - loss: 4.0987 - val_loss: 4.1052 - 70s/epoch - 7s/step
Epoch 78/100
10/10 - 69s - loss: 2.0528 - val_loss: 2.0234 - 69s/epoch - 7s/step
Epoch 79/100
10/10 - 70s - loss: 1.7526 - val_loss: 1.7445 - 70s/epoch - 7s/step
Epoch 80/100
10/10 - 69s - loss: 1.7429 - val_loss: 1.7338 - 69s/epoch - 7s/step
Epoch 81/100
10/10 - 67s - loss: 1.7761 - val_loss: 1.7835 - 67s/epoch - 7s/step
Epoch 82/100
10/10 - 71s - loss: 3.4985 - val_loss: 3.4989 - 71s/epoch - 7s/step
Epoch 83/100
10/10 - 70s - loss: 0.8434 - val_loss: 0.8429 - 70s/epoch - 7s/step
Epoch 84/100
10/10 - 68s - loss: 2.4450 - val_loss: 2.3715 - 68s/epoch - 7s/step
Epoch 85/100
10/10 - 70s - loss: 1.8198 - val_loss: 1.8165 - 70s/epoch - 7s/step
Epoch 86/100
10/10 - 70s - loss: 1.8900 - val_loss: 1.8960 - 70s/epoch - 7s/step
Epoch 87/100
10/10 - 71s - loss: 3.4378 - val_loss: 3.4322 - 71s/epoch - 7s/step
Epoch 88/100
10/10 - 71s - loss: 4.6617 - val_loss: 4.6721 - 71s/epoch - 7s/step
Epoch 89/100
10/10 - 69s - loss: 1.5401 - val_loss: 1.5786 - 69s/epoch - 7s/step
Epoch 90/100
10/10 - 68s - loss: 2.8537 - val_loss: 2.8237 - 68s/epoch - 7s/step
Epoch 91/100
10/10 - 70s - loss: 3.0848 - val_loss: 3.0445 - 70s/epoch - 7s/step
Epoch 92/100
10/10 - 69s - loss: 2.1600 - val_loss: 2.1575 - 69s/epoch - 7s/step
Epoch 93/100
10/10 - 69s - loss: 2.3714 - val_loss: 2.3624 - 69s/epoch - 7s/step
Epoch 94/100
10/10 - 70s - loss: 2.1706 - val_loss: 2.1744 - 70s/epoch - 7s/step
Epoch 95/100
10/10 - 67s - loss: 1.3448 - val_loss: 1.3365 - 67s/epoch - 7s/step
Epoch 96/100
10/10 - 69s - loss: 2.9872 - val_loss: 2.9955 - 69s/epoch - 7s/step
Epoch 97/100
10/10 - 70s - loss: 5.0119 - val_loss: 4.9109 - 70s/epoch - 7s/step
Epoch 98/100
10/10 - 71s - loss: 2.3708 - val_loss: 2.0301 - 71s/epoch - 7s/step
Epoch 99/100
10/10 - 71s - loss: 3.8666 - val_loss: 3.9288 - 71s/epoch - 7s/step
Epoch 100/100
10/10 - 71s - loss: 2.2012 - val_loss: 2.1894 - 71s/epoch - 7s/step
