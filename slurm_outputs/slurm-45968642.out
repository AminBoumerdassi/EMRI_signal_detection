
The following modules were not unloaded:
   (Use "module --force purge" to unload all):

  1) XALT/minimal   2) slurm   3) NeSI
2024-04-30 03:41:19.925134: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-30 03:41:19.925191: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-30 03:41:19.925225: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-30 03:41:19.936377: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-30 03:41:24.124166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38298 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:83:00.0, compute capability: 8.0
1 Physical GPUs, 1 Logical GPUs
#################################
####DATA GENERATOR PARAMETERS####
#Dataset size:  11011
#Batch size:  8
#Time in years: 2.658143162020893
#n_channels:  2
#dt:  10
#Length of timeseries: 8388608
Noise background:  False
#################################
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d (Conv1D)             (None, 1048576, 64)       8256      
                                                                 
 leaky_re_lu (LeakyReLU)     (None, 1048576, 64)       0         
                                                                 
 conv1d_1 (Conv1D)           (None, 131072, 64)        262208    
                                                                 
 leaky_re_lu_1 (LeakyReLU)   (None, 131072, 64)        0         
                                                                 
 conv1d_2 (Conv1D)           (None, 16384, 64)         262208    
                                                                 
 leaky_re_lu_2 (LeakyReLU)   (None, 16384, 64)         0         
                                                                 
 batch_normalization (Batch  (None, 16384, 64)         256       
 Normalization)                                                  
                                                                 
 conv1d_transpose (Conv1DTr  (None, 131072, 64)        262208    
 anspose)                                                        
                                                                 
 leaky_re_lu_3 (LeakyReLU)   (None, 131072, 64)        0         
                                                                 
 conv1d_transpose_1 (Conv1D  (None, 1048576, 64)       262208    
 Transpose)                                                      
                                                                 
 leaky_re_lu_4 (LeakyReLU)   (None, 1048576, 64)       0         
                                                                 
 conv1d_transpose_2 (Conv1D  (None, 8388608, 2)        8194      
 Transpose)                                                      
                                                                 
 activation (Activation)     (None, 8388608, 2)        0         
                                                                 
=================================================================
Total params: 1065538 (4.06 MB)
Trainable params: 1065410 (4.06 MB)
Non-trainable params: 128 (512.00 Byte)
_________________________________________________________________
Epoch 1/100
2024-04-30 03:41:55.240982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600
2024-04-30 03:43:11.504445: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2aac0c62ddd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-30 03:43:11.504508: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2024-04-30 03:43:11.818555: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
10/10 - 141s - loss: 3.2232 - val_loss: 3.2227 - 141s/epoch - 14s/step
Epoch 2/100
10/10 - 67s - loss: 2.6096 - val_loss: 2.6109 - 67s/epoch - 7s/step
Epoch 3/100
10/10 - 68s - loss: 2.3078 - val_loss: 2.3007 - 68s/epoch - 7s/step
Epoch 4/100
10/10 - 67s - loss: 1.5847 - val_loss: 1.5858 - 67s/epoch - 7s/step
Epoch 5/100
10/10 - 67s - loss: 4.6634 - val_loss: 4.6616 - 67s/epoch - 7s/step
Epoch 6/100
10/10 - 68s - loss: 4.0595 - val_loss: 4.0585 - 68s/epoch - 7s/step
Epoch 7/100
10/10 - 66s - loss: 1.7325 - val_loss: 1.7342 - 66s/epoch - 7s/step
Epoch 8/100
10/10 - 66s - loss: 3.7664 - val_loss: 3.7691 - 66s/epoch - 7s/step
Epoch 9/100
10/10 - 66s - loss: 4.7197 - val_loss: 4.6637 - 66s/epoch - 7s/step
Epoch 10/100
10/10 - 66s - loss: 4.0252 - val_loss: 3.6269 - 66s/epoch - 7s/step
Epoch 11/100
10/10 - 74s - loss: 2.4494 - val_loss: 2.4528 - 74s/epoch - 7s/step
Epoch 12/100
10/10 - 67s - loss: 4.3366 - val_loss: 4.3425 - 67s/epoch - 7s/step
Epoch 13/100
10/10 - 65s - loss: 2.2140 - val_loss: 2.2351 - 65s/epoch - 7s/step
Epoch 14/100
10/10 - 68s - loss: 2.9590 - val_loss: 2.9791 - 68s/epoch - 7s/step
Epoch 15/100
10/10 - 68s - loss: 3.6542 - val_loss: 3.6481 - 68s/epoch - 7s/step
Epoch 16/100
10/10 - 68s - loss: 3.2881 - val_loss: 3.1126 - 68s/epoch - 7s/step
Epoch 17/100
10/10 - 64s - loss: 3.5853 - val_loss: 3.5871 - 64s/epoch - 6s/step
Epoch 18/100
10/10 - 67s - loss: 3.8759 - val_loss: 4.2760 - 67s/epoch - 7s/step
Epoch 19/100
10/10 - 67s - loss: 2.6976 - val_loss: 2.6929 - 67s/epoch - 7s/step
Epoch 20/100
10/10 - 68s - loss: 2.7393 - val_loss: 2.8236 - 68s/epoch - 7s/step
Epoch 21/100
10/10 - 65s - loss: 2.1384 - val_loss: 2.1345 - 65s/epoch - 7s/step
Epoch 22/100
10/10 - 66s - loss: 3.0798 - val_loss: 3.0805 - 66s/epoch - 7s/step
Epoch 23/100
10/10 - 65s - loss: 3.0879 - val_loss: 3.0849 - 65s/epoch - 6s/step
Epoch 24/100
10/10 - 67s - loss: 3.1052 - val_loss: 3.0827 - 67s/epoch - 7s/step
Epoch 25/100
10/10 - 65s - loss: 2.9462 - val_loss: 2.9916 - 65s/epoch - 6s/step
Epoch 26/100
10/10 - 65s - loss: 3.7757 - val_loss: 3.7823 - 65s/epoch - 7s/step
Epoch 27/100
10/10 - 67s - loss: 3.5472 - val_loss: 3.4900 - 67s/epoch - 7s/step
Epoch 28/100
10/10 - 68s - loss: 2.7155 - val_loss: 2.7210 - 68s/epoch - 7s/step
Epoch 29/100
10/10 - 68s - loss: 3.3734 - val_loss: 3.3673 - 68s/epoch - 7s/step
Epoch 30/100
10/10 - 66s - loss: 3.5651 - val_loss: 3.5708 - 66s/epoch - 7s/step
Epoch 31/100
10/10 - 67s - loss: 3.6254 - val_loss: 3.6859 - 67s/epoch - 7s/step
Epoch 32/100
10/10 - 67s - loss: 2.9359 - val_loss: 2.9314 - 67s/epoch - 7s/step
Epoch 33/100
10/10 - 66s - loss: 2.9828 - val_loss: 2.9898 - 66s/epoch - 7s/step
Epoch 34/100
10/10 - 65s - loss: 2.9352 - val_loss: 2.9323 - 65s/epoch - 7s/step
Epoch 35/100
10/10 - 67s - loss: 3.7817 - val_loss: 3.8024 - 67s/epoch - 7s/step
Epoch 36/100
10/10 - 65s - loss: 3.0625 - val_loss: 3.0642 - 65s/epoch - 6s/step
Epoch 37/100
10/10 - 67s - loss: 2.7966 - val_loss: 2.7952 - 67s/epoch - 7s/step
Epoch 38/100
10/10 - 67s - loss: 2.4872 - val_loss: 2.4547 - 67s/epoch - 7s/step
Epoch 39/100
10/10 - 64s - loss: 3.9238 - val_loss: 3.9078 - 64s/epoch - 6s/step
Epoch 40/100
10/10 - 64s - loss: 3.6525 - val_loss: 3.5400 - 64s/epoch - 6s/step
Epoch 41/100
10/10 - 69s - loss: 4.1971 - val_loss: 4.1987 - 69s/epoch - 7s/step
Epoch 42/100
10/10 - 67s - loss: 3.0304 - val_loss: 3.0258 - 67s/epoch - 7s/step
Epoch 43/100
10/10 - 65s - loss: 2.4803 - val_loss: 2.4158 - 65s/epoch - 6s/step
Epoch 44/100
10/10 - 68s - loss: 3.0390 - val_loss: 3.0283 - 68s/epoch - 7s/step
Epoch 45/100
10/10 - 67s - loss: 3.1244 - val_loss: 3.1150 - 67s/epoch - 7s/step
Epoch 46/100
10/10 - 66s - loss: 1.9366 - val_loss: 1.9279 - 66s/epoch - 7s/step
Epoch 47/100
10/10 - 66s - loss: 3.3708 - val_loss: 3.4674 - 66s/epoch - 7s/step
Epoch 48/100
10/10 - 68s - loss: 3.1867 - val_loss: 3.1869 - 68s/epoch - 7s/step
Epoch 49/100
10/10 - 67s - loss: 2.6182 - val_loss: 2.6269 - 67s/epoch - 7s/step
Epoch 50/100
10/10 - 66s - loss: 2.9614 - val_loss: 2.9645 - 66s/epoch - 7s/step
Epoch 51/100
10/10 - 68s - loss: 3.6294 - val_loss: 3.6286 - 68s/epoch - 7s/step
Epoch 52/100
10/10 - 69s - loss: 2.9272 - val_loss: 2.9312 - 69s/epoch - 7s/step
Epoch 53/100
10/10 - 65s - loss: 2.9745 - val_loss: 2.9870 - 65s/epoch - 6s/step
Epoch 54/100
10/10 - 65s - loss: 2.9097 - val_loss: 3.0541 - 65s/epoch - 6s/step
Epoch 55/100
10/10 - 67s - loss: 2.2282 - val_loss: 2.2256 - 67s/epoch - 7s/step
Epoch 56/100
10/10 - 65s - loss: 2.3517 - val_loss: 2.3427 - 65s/epoch - 7s/step
Epoch 57/100
10/10 - 68s - loss: 3.3773 - val_loss: 3.4117 - 68s/epoch - 7s/step
Epoch 58/100
10/10 - 68s - loss: 3.4569 - val_loss: 3.4652 - 68s/epoch - 7s/step
Epoch 59/100
10/10 - 67s - loss: 3.5467 - val_loss: 3.5498 - 67s/epoch - 7s/step
Epoch 60/100
10/10 - 67s - loss: 2.1431 - val_loss: 2.1453 - 67s/epoch - 7s/step
Epoch 61/100
10/10 - 67s - loss: 3.0296 - val_loss: 3.0324 - 67s/epoch - 7s/step
Epoch 62/100
10/10 - 67s - loss: 3.5390 - val_loss: 3.5342 - 67s/epoch - 7s/step
Epoch 63/100
10/10 - 69s - loss: 3.5612 - val_loss: 3.5699 - 69s/epoch - 7s/step
Epoch 64/100
10/10 - 68s - loss: 3.8548 - val_loss: 3.8617 - 68s/epoch - 7s/step
Epoch 65/100
10/10 - 66s - loss: 2.7020 - val_loss: 2.6977 - 66s/epoch - 7s/step
Epoch 66/100
10/10 - 67s - loss: 4.0323 - val_loss: 4.0110 - 67s/epoch - 7s/step
Epoch 67/100
10/10 - 66s - loss: 3.2375 - val_loss: 3.2383 - 66s/epoch - 7s/step
Epoch 68/100
10/10 - 67s - loss: 2.3325 - val_loss: 2.3347 - 67s/epoch - 7s/step
Epoch 69/100
10/10 - 67s - loss: 3.2687 - val_loss: 3.2698 - 67s/epoch - 7s/step
Epoch 70/100
10/10 - 67s - loss: 2.9236 - val_loss: 2.9275 - 67s/epoch - 7s/step
Epoch 71/100
10/10 - 67s - loss: 2.6115 - val_loss: 2.6106 - 67s/epoch - 7s/step
Epoch 72/100
10/10 - 67s - loss: 2.2832 - val_loss: 2.2789 - 67s/epoch - 7s/step
Epoch 73/100
10/10 - 67s - loss: 2.5857 - val_loss: 2.5848 - 67s/epoch - 7s/step
Epoch 74/100
10/10 - 70s - loss: 3.5997 - val_loss: 3.6178 - 70s/epoch - 7s/step
Epoch 75/100
10/10 - 68s - loss: 5.1766 - val_loss: 5.4094 - 68s/epoch - 7s/step
Epoch 76/100
10/10 - 67s - loss: 3.3011 - val_loss: 3.3006 - 67s/epoch - 7s/step
Epoch 77/100
10/10 - 68s - loss: 4.8387 - val_loss: 4.9290 - 68s/epoch - 7s/step
Epoch 78/100
10/10 - 67s - loss: 2.7700 - val_loss: 2.7679 - 67s/epoch - 7s/step
Epoch 79/100
10/10 - 68s - loss: 2.5478 - val_loss: 2.5409 - 68s/epoch - 7s/step
Epoch 80/100
10/10 - 66s - loss: 2.4519 - val_loss: 2.4507 - 66s/epoch - 7s/step
Epoch 81/100
10/10 - 65s - loss: 2.5044 - val_loss: 2.5093 - 65s/epoch - 6s/step
Epoch 82/100
10/10 - 68s - loss: 4.3191 - val_loss: 4.3005 - 68s/epoch - 7s/step
Epoch 83/100
10/10 - 67s - loss: 1.5550 - val_loss: 1.5563 - 67s/epoch - 7s/step
Epoch 84/100
10/10 - 65s - loss: 3.0917 - val_loss: 3.0919 - 65s/epoch - 7s/step
Epoch 85/100
10/10 - 67s - loss: 2.6658 - val_loss: 2.6732 - 67s/epoch - 7s/step
Epoch 86/100
10/10 - 67s - loss: 2.7665 - val_loss: 2.7640 - 67s/epoch - 7s/step
Epoch 87/100
10/10 - 69s - loss: 4.2360 - val_loss: 4.2311 - 69s/epoch - 7s/step
Epoch 88/100
10/10 - 68s - loss: 5.4267 - val_loss: 5.4269 - 68s/epoch - 7s/step
Epoch 89/100
10/10 - 67s - loss: 2.3458 - val_loss: 2.3430 - 67s/epoch - 7s/step
Epoch 90/100
10/10 - 66s - loss: 3.7146 - val_loss: 3.7379 - 66s/epoch - 7s/step
Epoch 91/100
10/10 - 68s - loss: 3.8530 - val_loss: 3.8624 - 68s/epoch - 7s/step
Epoch 92/100
10/10 - 66s - loss: 2.9712 - val_loss: 2.9690 - 66s/epoch - 7s/step
Epoch 93/100
10/10 - 67s - loss: 3.2297 - val_loss: 3.2296 - 67s/epoch - 7s/step
Epoch 94/100
10/10 - 67s - loss: 3.0076 - val_loss: 3.0106 - 67s/epoch - 7s/step
Epoch 95/100
10/10 - 65s - loss: 2.0609 - val_loss: 2.0632 - 65s/epoch - 6s/step
Epoch 96/100
10/10 - 67s - loss: 3.6324 - val_loss: 3.6263 - 67s/epoch - 7s/step
Epoch 97/100
10/10 - 68s - loss: 5.8493 - val_loss: 5.5990 - 68s/epoch - 7s/step
Epoch 98/100
10/10 - 68s - loss: 3.0052 - val_loss: 2.7938 - 68s/epoch - 7s/step
Epoch 99/100
10/10 - 68s - loss: 4.6938 - val_loss: 4.7093 - 68s/epoch - 7s/step
Epoch 100/100
10/10 - 68s - loss: 2.8671 - val_loss: 3.0113 - 68s/epoch - 7s/step
