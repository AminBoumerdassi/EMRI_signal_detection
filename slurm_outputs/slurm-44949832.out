
The following modules were not unloaded:
   (Use "module --force purge" to unload all):

  1) XALT/minimal   2) slurm   3) NeSI
2024-03-25 14:48:07.175715: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-25 14:48:07.175790: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-25 14:48:07.175819: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-25 14:48:07.185322: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-25 14:48:10.591944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38298 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:05:00.0, compute capability: 8.0
1 Physical GPUs, 1 Logical GPUs
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d (Conv1D)             (None, 1048576, 32)       8224      
                                                                 
 conv1d_1 (Conv1D)           (None, 262144, 16)        65552     
                                                                 
 conv1d_2 (Conv1D)           (None, 65536, 8)          16392     
                                                                 
 conv1d_transpose (Conv1DTr  (None, 262144, 8)         8200      
 anspose)                                                        
                                                                 
 conv1d_transpose_1 (Conv1D  (None, 1048576, 16)       16400     
 Transpose)                                                      
                                                                 
 conv1d_transpose_2 (Conv1D  (None, 4194304, 32)       65568     
 Transpose)                                                      
                                                                 
 conv1d_transpose_3 (Conv1D  (None, 4194304, 2)        66        
 Transpose)                                                      
                                                                 
 activation (Activation)     (None, 4194304, 2)        0         
                                                                 
=================================================================
Total params: 180402 (704.70 KB)
Trainable params: 180402 (704.70 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
#################################
####DATA GENERATOR PARAMETERS####
#Batch size:  8
#Time in years: 1.3290715810104465
#n_channels:  2
#dt:  10
#Length of timeseries: 4194304
Noise background:  False
#################################
Epoch 1/150
2024-03-25 14:48:29.569519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600
2024-03-25 14:51:10.929488: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ab208656840 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-03-25 14:51:10.930986: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2024-03-25 14:51:10.939112: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-03-25 14:51:12.466099: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/1 - 175s - loss: 4.0764e-06 - val_loss: 4.0765e-06 - 175s/epoch - 175s/step
Epoch 2/150
1/1 - 4s - loss: 1.0056e-04 - val_loss: 1.0695e-04 - 4s/epoch - 4s/step
Epoch 3/150
1/1 - 5s - loss: 2.0564e-05 - val_loss: 1.7014e-05 - 5s/epoch - 5s/step
Epoch 4/150
1/1 - 4s - loss: 5.8437e-06 - val_loss: 9.0443e-06 - 4s/epoch - 4s/step
Epoch 5/150
1/1 - 4s - loss: 2.6171e-05 - val_loss: 2.0892e-05 - 4s/epoch - 4s/step
Epoch 6/150
1/1 - 4s - loss: 1.5668e-05 - val_loss: 1.6292e-05 - 4s/epoch - 4s/step
Epoch 7/150
1/1 - 5s - loss: 2.0342e-05 - val_loss: 2.2683e-05 - 5s/epoch - 5s/step
Epoch 8/150
1/1 - 4s - loss: 6.7890e-06 - val_loss: 4.7167e-06 - 4s/epoch - 4s/step
Epoch 9/150
1/1 - 4s - loss: 2.3253e-05 - val_loss: 2.1705e-05 - 4s/epoch - 4s/step
Epoch 10/150
1/1 - 4s - loss: 1.5057e-05 - val_loss: 1.6514e-05 - 4s/epoch - 4s/step
Epoch 11/150
1/1 - 4s - loss: 9.2761e-06 - val_loss: 9.8948e-06 - 4s/epoch - 4s/step
Epoch 12/150
1/1 - 4s - loss: 7.9827e-06 - val_loss: 6.5739e-06 - 4s/epoch - 4s/step
Epoch 13/150
1/1 - 4s - loss: 1.8386e-05 - val_loss: 1.7718e-05 - 4s/epoch - 4s/step
Epoch 14/150
1/1 - 4s - loss: 3.5792e-06 - val_loss: 4.4817e-06 - 4s/epoch - 4s/step
Epoch 15/150
1/1 - 4s - loss: 1.0562e-05 - val_loss: 1.0939e-05 - 4s/epoch - 4s/step
Epoch 16/150
1/1 - 4s - loss: 7.5302e-05 - val_loss: 7.4454e-05 - 4s/epoch - 4s/step
Epoch 17/150
1/1 - 4s - loss: 5.8884e-06 - val_loss: 5.4071e-06 - 4s/epoch - 4s/step
Epoch 18/150
1/1 - 4s - loss: 3.1699e-06 - val_loss: 3.6969e-06 - 4s/epoch - 4s/step
Epoch 19/150
1/1 - 4s - loss: 6.7397e-06 - val_loss: 7.0608e-06 - 4s/epoch - 4s/step
Epoch 20/150
1/1 - 4s - loss: 9.2219e-06 - val_loss: 8.7341e-06 - 4s/epoch - 4s/step
Epoch 21/150
1/1 - 4s - loss: 5.7069e-06 - val_loss: 5.3214e-06 - 4s/epoch - 4s/step
Epoch 22/150
1/1 - 4s - loss: 8.3257e-06 - val_loss: 8.6132e-06 - 4s/epoch - 4s/step
Epoch 23/150
1/1 - 4s - loss: 7.7476e-06 - val_loss: 8.0173e-06 - 4s/epoch - 4s/step
Epoch 24/150
1/1 - 4s - loss: 8.4160e-06 - val_loss: 8.1378e-06 - 4s/epoch - 4s/step
Epoch 25/150
1/1 - 4s - loss: 1.3867e-05 - val_loss: 1.3575e-05 - 4s/epoch - 4s/step
Epoch 26/150
1/1 - 4s - loss: 1.3387e-05 - val_loss: 1.3543e-05 - 4s/epoch - 4s/step
Epoch 27/150
1/1 - 4s - loss: 7.1081e-06 - val_loss: 7.3091e-06 - 4s/epoch - 4s/step
Epoch 28/150
1/1 - 4s - loss: 3.9444e-06 - val_loss: 3.7903e-06 - 4s/epoch - 4s/step
Epoch 29/150
1/1 - 4s - loss: 1.4077e-05 - val_loss: 1.3866e-05 - 4s/epoch - 4s/step
Epoch 30/150
1/1 - 5s - loss: 5.6811e-05 - val_loss: 5.6896e-05 - 5s/epoch - 5s/step
Epoch 31/150
1/1 - 4s - loss: 7.5395e-06 - val_loss: 7.6823e-06 - 4s/epoch - 4s/step
Epoch 32/150
1/1 - 4s - loss: 1.8752e-05 - val_loss: 1.8656e-05 - 4s/epoch - 4s/step
Epoch 33/150
1/1 - 4s - loss: 9.6873e-06 - val_loss: 9.5524e-06 - 4s/epoch - 4s/step
Epoch 34/150
1/1 - 4s - loss: 3.2394e-05 - val_loss: 3.2449e-05 - 4s/epoch - 4s/step
Epoch 35/150
1/1 - 4s - loss: 4.9366e-06 - val_loss: 5.0190e-06 - 4s/epoch - 4s/step
Epoch 36/150
1/1 - 4s - loss: 1.0879e-05 - val_loss: 1.0807e-05 - 4s/epoch - 4s/step
Epoch 37/150
1/1 - 4s - loss: 1.0442e-05 - val_loss: 1.0364e-05 - 4s/epoch - 4s/step
Epoch 38/150
1/1 - 4s - loss: 5.1646e-06 - val_loss: 5.2188e-06 - 4s/epoch - 4s/step
Epoch 39/150
1/1 - 4s - loss: 1.4081e-05 - val_loss: 1.4124e-05 - 4s/epoch - 4s/step
Epoch 40/150
1/1 - 4s - loss: 1.7782e-05 - val_loss: 1.7718e-05 - 4s/epoch - 4s/step
Epoch 41/150
1/1 - 4s - loss: 3.0602e-06 - val_loss: 3.0186e-06 - 4s/epoch - 4s/step
Epoch 42/150
1/1 - 4s - loss: 2.4740e-05 - val_loss: 2.4782e-05 - 4s/epoch - 4s/step
Epoch 43/150
1/1 - 4s - loss: 1.7523e-04 - val_loss: 1.7524e-04 - 4s/epoch - 4s/step
Epoch 44/150
1/1 - 4s - loss: 3.6705e-06 - val_loss: 3.6250e-06 - 4s/epoch - 4s/step
Epoch 45/150
1/1 - 4s - loss: 8.5701e-06 - val_loss: 8.5587e-06 - 4s/epoch - 4s/step
Epoch 46/150
1/1 - 4s - loss: 3.5868e-06 - val_loss: 3.6162e-06 - 4s/epoch - 4s/step
Epoch 47/150
1/1 - 4s - loss: 1.2386e-05 - val_loss: 1.2378e-05 - 4s/epoch - 4s/step
Epoch 48/150
1/1 - 4s - loss: 1.1176e-05 - val_loss: 1.1148e-05 - 4s/epoch - 4s/step
Epoch 49/150
1/1 - 4s - loss: 1.2904e-05 - val_loss: 1.2913e-05 - 4s/epoch - 4s/step
Epoch 50/150
1/1 - 4s - loss: 4.7109e-06 - val_loss: 4.7246e-06 - 4s/epoch - 4s/step
Epoch 51/150
1/1 - 4s - loss: 3.9334e-06 - val_loss: 3.9140e-06 - 4s/epoch - 4s/step
Epoch 52/150
1/1 - 4s - loss: 2.6613e-06 - val_loss: 2.6510e-06 - 4s/epoch - 4s/step
Epoch 53/150
1/1 - 4s - loss: 2.5476e-06 - val_loss: 2.5629e-06 - 4s/epoch - 4s/step
Epoch 54/150
1/1 - 4s - loss: 1.0042e-04 - val_loss: 1.0041e-04 - 4s/epoch - 4s/step
Epoch 55/150
1/1 - 4s - loss: 6.6877e-06 - val_loss: 6.6714e-06 - 4s/epoch - 4s/step
Epoch 56/150
1/1 - 4s - loss: 9.5439e-06 - val_loss: 9.5470e-06 - 4s/epoch - 4s/step
Epoch 57/150
1/1 - 4s - loss: 3.3334e-06 - val_loss: 3.3423e-06 - 4s/epoch - 4s/step
Epoch 58/150
1/1 - 4s - loss: 3.4212e-06 - val_loss: 3.4113e-06 - 4s/epoch - 4s/step
Epoch 59/150
1/1 - 4s - loss: 5.2620e-06 - val_loss: 5.2551e-06 - 4s/epoch - 4s/step
Epoch 60/150
1/1 - 4s - loss: 3.4766e-06 - val_loss: 3.4856e-06 - 4s/epoch - 4s/step
Epoch 61/150
1/1 - 4s - loss: 5.0426e-06 - val_loss: 5.0432e-06 - 4s/epoch - 4s/step
Epoch 62/150
1/1 - 4s - loss: 7.6051e-06 - val_loss: 7.5951e-06 - 4s/epoch - 4s/step
Epoch 63/150
1/1 - 5s - loss: 3.4064e-06 - val_loss: 3.4078e-06 - 5s/epoch - 5s/step
Epoch 64/150
1/1 - 4s - loss: 2.3854e-04 - val_loss: 2.3852e-04 - 4s/epoch - 4s/step
Epoch 65/150
1/1 - 4s - loss: 3.8731e-06 - val_loss: 3.8682e-06 - 4s/epoch - 4s/step
Epoch 66/150
1/1 - 4s - loss: 4.0709e-06 - val_loss: 4.0664e-06 - 4s/epoch - 4s/step
Epoch 67/150
1/1 - 4s - loss: 7.4182e-06 - val_loss: 7.4221e-06 - 4s/epoch - 4s/step
Epoch 68/150
1/1 - 4s - loss: 6.9910e-05 - val_loss: 6.9906e-05 - 4s/epoch - 4s/step
Epoch 69/150
1/1 - 4s - loss: 2.5709e-06 - val_loss: 2.5666e-06 - 4s/epoch - 4s/step
Epoch 70/150
1/1 - 4s - loss: 9.0834e-06 - val_loss: 9.0834e-06 - 4s/epoch - 4s/step
Epoch 71/150
1/1 - 4s - loss: 3.2834e-05 - val_loss: 3.2834e-05 - 4s/epoch - 4s/step
Epoch 72/150
1/1 - 4s - loss: 1.2282e-05 - val_loss: 1.2279e-05 - 4s/epoch - 4s/step
Epoch 73/150
1/1 - 4s - loss: 7.6907e-06 - val_loss: 7.6894e-06 - 4s/epoch - 4s/step
Epoch 74/150
1/1 - 4s - loss: 6.2468e-06 - val_loss: 6.2480e-06 - 4s/epoch - 4s/step
Epoch 75/150
1/1 - 4s - loss: 2.7544e-06 - val_loss: 2.7537e-06 - 4s/epoch - 4s/step
Epoch 76/150
1/1 - 4s - loss: 1.8046e-05 - val_loss: 1.8043e-05 - 4s/epoch - 4s/step
Epoch 77/150
1/1 - 4s - loss: 7.7399e-06 - val_loss: 7.7405e-06 - 4s/epoch - 4s/step
Epoch 78/150
1/1 - 5s - loss: 2.9390e-05 - val_loss: 2.9387e-05 - 5s/epoch - 5s/step
Epoch 79/150
1/1 - 4s - loss: 2.0942e-05 - val_loss: 2.0939e-05 - 4s/epoch - 4s/step
Epoch 80/150
1/1 - 4s - loss: 2.6106e-05 - val_loss: 2.6105e-05 - 4s/epoch - 4s/step
Epoch 81/150
1/1 - 4s - loss: 9.4592e-06 - val_loss: 9.4590e-06 - 4s/epoch - 4s/step
Epoch 82/150
1/1 - 4s - loss: 9.2053e-06 - val_loss: 9.2031e-06 - 4s/epoch - 4s/step
Epoch 83/150
1/1 - 4s - loss: 2.2176e-05 - val_loss: 2.2174e-05 - 4s/epoch - 4s/step
Epoch 84/150
1/1 - 4s - loss: 1.5990e-05 - val_loss: 1.5989e-05 - 4s/epoch - 4s/step
Epoch 85/150
1/1 - 4s - loss: 7.8784e-06 - val_loss: 7.8773e-06 - 4s/epoch - 4s/step
Epoch 86/150
1/1 - 4s - loss: 5.3998e-06 - val_loss: 5.3985e-06 - 4s/epoch - 4s/step
Epoch 87/150
1/1 - 4s - loss: 7.2572e-06 - val_loss: 7.2571e-06 - 4s/epoch - 4s/step
Epoch 88/150
1/1 - 4s - loss: 5.0368e-06 - val_loss: 5.0368e-06 - 4s/epoch - 4s/step
Epoch 89/150
1/1 - 4s - loss: 1.0514e-05 - val_loss: 1.0512e-05 - 4s/epoch - 4s/step
Epoch 90/150
1/1 - 4s - loss: 2.8826e-05 - val_loss: 2.8825e-05 - 4s/epoch - 4s/step
Epoch 91/150
1/1 - 4s - loss: 1.5230e-05 - val_loss: 1.5229e-05 - 4s/epoch - 4s/step
Epoch 92/150
1/1 - 4s - loss: 2.4965e-06 - val_loss: 2.4960e-06 - 4s/epoch - 4s/step
Epoch 93/150
1/1 - 4s - loss: 1.0128e-05 - val_loss: 1.0127e-05 - 4s/epoch - 4s/step
Epoch 94/150
1/1 - 4s - loss: 1.0499e-05 - val_loss: 1.0498e-05 - 4s/epoch - 4s/step
Epoch 95/150
1/1 - 5s - loss: 1.2961e-04 - val_loss: 1.2960e-04 - 5s/epoch - 5s/step
Epoch 96/150
1/1 - 5s - loss: 1.4618e-04 - val_loss: 1.4617e-04 - 5s/epoch - 5s/step
Epoch 97/150
1/1 - 4s - loss: 2.0432e-05 - val_loss: 2.0430e-05 - 4s/epoch - 4s/step
Epoch 98/150
1/1 - 4s - loss: 2.7801e-05 - val_loss: 2.7796e-05 - 4s/epoch - 4s/step
Epoch 99/150
1/1 - 4s - loss: 2.3902e-04 - val_loss: 2.3899e-04 - 4s/epoch - 4s/step
Epoch 100/150
1/1 - 4s - loss: 1.2283e-05 - val_loss: 1.2281e-05 - 4s/epoch - 4s/step
Epoch 101/150
1/1 - 4s - loss: 3.5533e-05 - val_loss: 3.5527e-05 - 4s/epoch - 4s/step
Epoch 102/150
1/1 - 5s - loss: 4.7143e-06 - val_loss: 4.7137e-06 - 5s/epoch - 5s/step
Epoch 103/150
1/1 - 4s - loss: 5.7500e-06 - val_loss: 5.7495e-06 - 4s/epoch - 4s/step
Epoch 104/150
1/1 - 4s - loss: 2.5253e-06 - val_loss: 2.5251e-06 - 4s/epoch - 4s/step
Epoch 105/150
1/1 - 4s - loss: 4.9213e-06 - val_loss: 4.9208e-06 - 4s/epoch - 4s/step
Epoch 106/150
1/1 - 4s - loss: 6.4522e-06 - val_loss: 6.4516e-06 - 4s/epoch - 4s/step
Epoch 107/150
1/1 - 4s - loss: 9.0687e-06 - val_loss: 9.0679e-06 - 4s/epoch - 4s/step
Epoch 108/150
1/1 - 4s - loss: 4.4395e-06 - val_loss: 4.4391e-06 - 4s/epoch - 4s/step
Epoch 109/150
1/1 - 4s - loss: 1.1740e-04 - val_loss: 1.1739e-04 - 4s/epoch - 4s/step
Epoch 110/150
1/1 - 4s - loss: 7.7649e-06 - val_loss: 7.7642e-06 - 4s/epoch - 4s/step
Epoch 111/150
1/1 - 4s - loss: 4.4142e-06 - val_loss: 4.4138e-06 - 4s/epoch - 4s/step
Epoch 112/150
1/1 - 4s - loss: 7.0884e-06 - val_loss: 7.0877e-06 - 4s/epoch - 4s/step
Epoch 113/150
1/1 - 4s - loss: 9.4260e-06 - val_loss: 9.4252e-06 - 4s/epoch - 4s/step
Epoch 114/150
1/1 - 4s - loss: 4.9617e-06 - val_loss: 4.9612e-06 - 4s/epoch - 4s/step
Epoch 115/150
1/1 - 4s - loss: 1.4794e-04 - val_loss: 1.4792e-04 - 4s/epoch - 4s/step
Epoch 116/150
1/1 - 4s - loss: 5.3444e-06 - val_loss: 5.3438e-06 - 4s/epoch - 4s/step
Epoch 117/150
1/1 - 4s - loss: 7.7702e-06 - val_loss: 7.7695e-06 - 4s/epoch - 4s/step
Epoch 118/150
1/1 - 4s - loss: 1.5140e-05 - val_loss: 1.5138e-05 - 4s/epoch - 4s/step
Epoch 119/150
1/1 - 4s - loss: 7.9270e-06 - val_loss: 7.9262e-06 - 4s/epoch - 4s/step
Epoch 120/150
1/1 - 5s - loss: 8.2393e-06 - val_loss: 8.2383e-06 - 5s/epoch - 5s/step
Epoch 121/150
1/1 - 4s - loss: 8.3962e-06 - val_loss: 8.3952e-06 - 4s/epoch - 4s/step
Epoch 122/150
1/1 - 4s - loss: 3.4192e-06 - val_loss: 3.4189e-06 - 4s/epoch - 4s/step
Epoch 123/150
1/1 - 4s - loss: 6.3812e-06 - val_loss: 6.3806e-06 - 4s/epoch - 4s/step
Epoch 124/150
1/1 - 4s - loss: 4.6872e-06 - val_loss: 4.6867e-06 - 4s/epoch - 4s/step
Epoch 125/150
1/1 - 4s - loss: 8.0212e-06 - val_loss: 8.0206e-06 - 4s/epoch - 4s/step
Epoch 126/150
1/1 - 4s - loss: 4.4662e-06 - val_loss: 4.4658e-06 - 4s/epoch - 4s/step
Epoch 127/150
1/1 - 4s - loss: 2.8865e-06 - val_loss: 2.8863e-06 - 4s/epoch - 4s/step
Epoch 128/150
1/1 - 4s - loss: 2.3798e-05 - val_loss: 2.3797e-05 - 4s/epoch - 4s/step
Epoch 129/150
1/1 - 5s - loss: 4.0807e-05 - val_loss: 4.0803e-05 - 5s/epoch - 5s/step
Epoch 130/150
1/1 - 4s - loss: 5.2740e-06 - val_loss: 5.2735e-06 - 4s/epoch - 4s/step
Epoch 131/150
1/1 - 4s - loss: 1.6502e-05 - val_loss: 1.6500e-05 - 4s/epoch - 4s/step
Epoch 132/150
1/1 - 4s - loss: 1.2729e-05 - val_loss: 1.2726e-05 - 4s/epoch - 4s/step
Epoch 133/150
1/1 - 4s - loss: 1.9997e-06 - val_loss: 1.9995e-06 - 4s/epoch - 4s/step
Epoch 134/150
1/1 - 4s - loss: 4.2733e-06 - val_loss: 4.2729e-06 - 4s/epoch - 4s/step
Epoch 135/150
1/1 - 4s - loss: 5.5465e-06 - val_loss: 5.5460e-06 - 4s/epoch - 4s/step
Epoch 136/150
1/1 - 4s - loss: 1.3179e-05 - val_loss: 1.3178e-05 - 4s/epoch - 4s/step
Epoch 137/150
1/1 - 4s - loss: 2.7372e-06 - val_loss: 2.7370e-06 - 4s/epoch - 4s/step
Epoch 138/150
1/1 - 4s - loss: 2.8264e-06 - val_loss: 2.8262e-06 - 4s/epoch - 4s/step
Epoch 139/150
1/1 - 4s - loss: 8.6047e-06 - val_loss: 8.6037e-06 - 4s/epoch - 4s/step
Epoch 140/150
1/1 - 4s - loss: 1.5159e-05 - val_loss: 1.5157e-05 - 4s/epoch - 4s/step
Epoch 141/150
1/1 - 4s - loss: 8.5224e-06 - val_loss: 8.5211e-06 - 4s/epoch - 4s/step
Epoch 142/150
1/1 - 4s - loss: 6.0537e-06 - val_loss: 6.0529e-06 - 4s/epoch - 4s/step
Epoch 143/150
1/1 - 4s - loss: 1.3052e-05 - val_loss: 1.3050e-05 - 4s/epoch - 4s/step
Epoch 144/150
1/1 - 4s - loss: 1.1249e-04 - val_loss: 1.1248e-04 - 4s/epoch - 4s/step
Epoch 145/150
1/1 - 4s - loss: 1.8739e-05 - val_loss: 1.8736e-05 - 4s/epoch - 4s/step
Epoch 146/150
1/1 - 4s - loss: 4.3897e-06 - val_loss: 4.3893e-06 - 4s/epoch - 4s/step
Epoch 147/150
1/1 - 4s - loss: 3.5419e-06 - val_loss: 3.5415e-06 - 4s/epoch - 4s/step
Epoch 148/150
1/1 - 4s - loss: 7.7984e-06 - val_loss: 7.7974e-06 - 4s/epoch - 4s/step
Epoch 149/150
1/1 - 4s - loss: 5.3724e-06 - val_loss: 5.3718e-06 - 4s/epoch - 4s/step
Epoch 150/150
1/1 - 4s - loss: 4.6287e-06 - val_loss: 4.6282e-06 - 4s/epoch - 4s/step
