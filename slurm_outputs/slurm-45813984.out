
The following modules were not unloaded:
   (Use "module --force purge" to unload all):

  1) XALT/minimal   2) slurm   3) NeSI
2024-04-23 20:12:34.543428: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-23 20:12:34.543512: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-23 20:12:34.638702: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-23 20:12:35.304530: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-23 20:13:06.228316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38298 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:83:00.0, compute capability: 8.0
1 Physical GPUs, 1 Logical GPUs
#################################
####DATA GENERATOR PARAMETERS####
#Dataset size:  11011
#Batch size:  8
#Time in years: 2.658143162020893
#n_channels:  2
#dt:  10
#Length of timeseries: 8388608
Noise background:  False
#################################
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d (Conv1D)             (None, 1048576, 64)       8256      
                                                                 
 conv1d_1 (Conv1D)           (None, 131072, 64)        262208    
                                                                 
 conv1d_2 (Conv1D)           (None, 16384, 64)         262208    
                                                                 
 conv1d_transpose (Conv1DTr  (None, 131072, 64)        262208    
 anspose)                                                        
                                                                 
 conv1d_transpose_1 (Conv1D  (None, 1048576, 64)       262208    
 Transpose)                                                      
                                                                 
 conv1d_transpose_2 (Conv1D  (None, 8388608, 2)        8194      
 Transpose)                                                      
                                                                 
 activation (Activation)     (None, 8388608, 2)        0         
                                                                 
=================================================================
Total params: 1065282 (4.06 MB)
Trainable params: 1065282 (4.06 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
Epoch 1/100
2024-04-23 20:13:42.585311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600
2024-04-23 20:15:05.254493: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ab1f6875110 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-23 20:15:05.254594: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2024-04-23 20:15:05.388637: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-04-23 20:15:05.709364: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
10/10 - 154s - loss: 0.9885 - val_loss: 0.9861 - 154s/epoch - 15s/step
Epoch 2/100
10/10 - 70s - loss: 0.9876 - val_loss: 0.9827 - 70s/epoch - 7s/step
Epoch 3/100
10/10 - 71s - loss: 0.9808 - val_loss: 0.9817 - 71s/epoch - 7s/step
Epoch 4/100
10/10 - 70s - loss: 0.9835 - val_loss: 0.9805 - 70s/epoch - 7s/step
Epoch 5/100
10/10 - 70s - loss: 0.9881 - val_loss: 0.9861 - 70s/epoch - 7s/step
Epoch 6/100
10/10 - 77s - loss: 0.9822 - val_loss: 0.9768 - 77s/epoch - 8s/step
Epoch 7/100
10/10 - 69s - loss: 0.9730 - val_loss: 0.9667 - 69s/epoch - 7s/step
Epoch 8/100
10/10 - 69s - loss: 0.9579 - val_loss: 0.9420 - 69s/epoch - 7s/step
Epoch 9/100
10/10 - 69s - loss: 0.8979 - val_loss: 0.8688 - 69s/epoch - 7s/step
Epoch 10/100
10/10 - 69s - loss: 0.8447 - val_loss: 0.8263 - 69s/epoch - 7s/step
Epoch 11/100
10/10 - 68s - loss: 0.8170 - val_loss: 0.8070 - 68s/epoch - 7s/step
Epoch 12/100
10/10 - 71s - loss: 0.8323 - val_loss: 0.8274 - 71s/epoch - 7s/step
Epoch 13/100
10/10 - 69s - loss: 0.7614 - val_loss: 0.7482 - 69s/epoch - 7s/step
Epoch 14/100
10/10 - 71s - loss: 0.7890 - val_loss: 0.7780 - 71s/epoch - 7s/step
Epoch 15/100
10/10 - 71s - loss: 0.7623 - val_loss: 0.7566 - 71s/epoch - 7s/step
Epoch 16/100
10/10 - 71s - loss: 0.7837 - val_loss: 0.7882 - 71s/epoch - 7s/step
Epoch 17/100
10/10 - 68s - loss: 0.7603 - val_loss: 0.7597 - 68s/epoch - 7s/step
Epoch 18/100
10/10 - 70s - loss: 0.7067 - val_loss: 0.6994 - 70s/epoch - 7s/step
Epoch 19/100
10/10 - 70s - loss: 0.7255 - val_loss: 0.7211 - 70s/epoch - 7s/step
Epoch 20/100
10/10 - 71s - loss: 0.7086 - val_loss: 0.7060 - 71s/epoch - 7s/step
Epoch 21/100
10/10 - 68s - loss: 0.6383 - val_loss: 0.6378 - 68s/epoch - 7s/step
Epoch 22/100
10/10 - 69s - loss: 0.6905 - val_loss: 0.6838 - 69s/epoch - 7s/step
Epoch 23/100
10/10 - 68s - loss: 0.6555 - val_loss: 0.6551 - 68s/epoch - 7s/step
Epoch 24/100
10/10 - 70s - loss: 0.6598 - val_loss: 0.6586 - 70s/epoch - 7s/step
Epoch 25/100
10/10 - 69s - loss: 0.6511 - val_loss: 0.6467 - 69s/epoch - 7s/step
Epoch 26/100
10/10 - 69s - loss: 0.6862 - val_loss: 0.6816 - 69s/epoch - 7s/step
Epoch 27/100
10/10 - 70s - loss: 0.7098 - val_loss: 0.7007 - 70s/epoch - 7s/step
Epoch 28/100
10/10 - 71s - loss: 0.6593 - val_loss: 0.6640 - 71s/epoch - 7s/step
Epoch 29/100
10/10 - 77s - loss: 0.6830 - val_loss: 0.6823 - 77s/epoch - 8s/step
Epoch 30/100
10/10 - 70s - loss: 0.6502 - val_loss: 0.6534 - 70s/epoch - 7s/step
Epoch 31/100
10/10 - 70s - loss: 0.6693 - val_loss: 0.6641 - 70s/epoch - 7s/step
Epoch 32/100
10/10 - 70s - loss: 0.6142 - val_loss: 0.6107 - 70s/epoch - 7s/step
Epoch 33/100
10/10 - 69s - loss: 0.6241 - val_loss: 0.6160 - 69s/epoch - 7s/step
Epoch 34/100
10/10 - 69s - loss: 0.6079 - val_loss: 0.6068 - 69s/epoch - 7s/step
Epoch 35/100
10/10 - 71s - loss: 0.5958 - val_loss: 0.6009 - 71s/epoch - 7s/step
Epoch 36/100
10/10 - 69s - loss: 0.6414 - val_loss: 0.6394 - 69s/epoch - 7s/step
Epoch 37/100
10/10 - 71s - loss: 0.6308 - val_loss: 0.6289 - 71s/epoch - 7s/step
Epoch 38/100
10/10 - 71s - loss: 0.6185 - val_loss: 0.6138 - 71s/epoch - 7s/step
Epoch 39/100
10/10 - 68s - loss: 0.5852 - val_loss: 0.5854 - 68s/epoch - 7s/step
Epoch 40/100
10/10 - 69s - loss: 0.6346 - val_loss: 0.6277 - 69s/epoch - 7s/step
Epoch 41/100
10/10 - 73s - loss: 0.6559 - val_loss: 0.6560 - 73s/epoch - 7s/step
Epoch 42/100
10/10 - 71s - loss: 0.6027 - val_loss: 0.5961 - 71s/epoch - 7s/step
Epoch 43/100
10/10 - 69s - loss: 0.5527 - val_loss: 0.5528 - 69s/epoch - 7s/step
Epoch 44/100
10/10 - 72s - loss: 0.6432 - val_loss: 0.6423 - 72s/epoch - 7s/step
Epoch 45/100
10/10 - 72s - loss: 0.6318 - val_loss: 0.6290 - 72s/epoch - 7s/step
Epoch 46/100
10/10 - 70s - loss: 0.5261 - val_loss: 0.5175 - 70s/epoch - 7s/step
Epoch 47/100
10/10 - 70s - loss: 0.5717 - val_loss: 0.5652 - 70s/epoch - 7s/step
Epoch 48/100
10/10 - 72s - loss: 0.6529 - val_loss: 0.6527 - 72s/epoch - 7s/step
Epoch 49/100
10/10 - 72s - loss: 0.5743 - val_loss: 0.5710 - 72s/epoch - 7s/step
Epoch 50/100
10/10 - 71s - loss: 0.5727 - val_loss: 0.5711 - 71s/epoch - 7s/step
Epoch 51/100
10/10 - 72s - loss: 0.5792 - val_loss: 0.5781 - 72s/epoch - 7s/step
Epoch 52/100
10/10 - 73s - loss: 0.6062 - val_loss: 0.6019 - 73s/epoch - 7s/step
Epoch 53/100
10/10 - 69s - loss: 0.5519 - val_loss: 0.5417 - 69s/epoch - 7s/step
Epoch 54/100
10/10 - 69s - loss: 0.5607 - val_loss: 0.5616 - 69s/epoch - 7s/step
Epoch 55/100
10/10 - 72s - loss: 0.5786 - val_loss: 0.5757 - 72s/epoch - 7s/step
Epoch 56/100
10/10 - 69s - loss: 0.5567 - val_loss: 0.5503 - 69s/epoch - 7s/step
Epoch 57/100
10/10 - 73s - loss: 0.6158 - val_loss: 0.6145 - 73s/epoch - 7s/step
Epoch 58/100
10/10 - 72s - loss: 0.5717 - val_loss: 0.5685 - 72s/epoch - 7s/step
Epoch 59/100
10/10 - 72s - loss: 0.6322 - val_loss: 0.6316 - 72s/epoch - 7s/step
Epoch 60/100
10/10 - 71s - loss: 0.5330 - val_loss: 0.5319 - 71s/epoch - 7s/step
Epoch 61/100
10/10 - 72s - loss: 0.6063 - val_loss: 0.6109 - 72s/epoch - 7s/step
Epoch 62/100
10/10 - 77s - loss: 0.6079 - val_loss: 0.6132 - 77s/epoch - 8s/step
Epoch 63/100
10/10 - 74s - loss: 0.6168 - val_loss: 0.6167 - 74s/epoch - 7s/step
Epoch 64/100
10/10 - 72s - loss: 0.6017 - val_loss: 0.6016 - 72s/epoch - 7s/step
Epoch 65/100
10/10 - 70s - loss: 0.5415 - val_loss: 0.5405 - 70s/epoch - 7s/step
Epoch 66/100
10/10 - 71s - loss: 0.5309 - val_loss: 0.5315 - 71s/epoch - 7s/step
Epoch 67/100
10/10 - 70s - loss: 0.5507 - val_loss: 0.5477 - 70s/epoch - 7s/step
Epoch 68/100
10/10 - 72s - loss: 0.4968 - val_loss: 0.4984 - 72s/epoch - 7s/step
Epoch 69/100
10/10 - 72s - loss: 0.6007 - val_loss: 0.5970 - 72s/epoch - 7s/step
Epoch 70/100
10/10 - 71s - loss: 0.5680 - val_loss: 0.5677 - 71s/epoch - 7s/step
Epoch 71/100
10/10 - 72s - loss: 0.5398 - val_loss: 0.5373 - 72s/epoch - 7s/step
Epoch 72/100
10/10 - 71s - loss: 0.5175 - val_loss: 0.5226 - 71s/epoch - 7s/step
Epoch 73/100
10/10 - 71s - loss: 0.5804 - val_loss: 0.5823 - 71s/epoch - 7s/step
Epoch 74/100
10/10 - 73s - loss: 0.5678 - val_loss: 0.5667 - 73s/epoch - 7s/step
Epoch 75/100
10/10 - 72s - loss: 0.6302 - val_loss: 0.6330 - 72s/epoch - 7s/step
Epoch 76/100
10/10 - 71s - loss: 0.5227 - val_loss: 0.5229 - 71s/epoch - 7s/step
Epoch 77/100
10/10 - 72s - loss: 0.6154 - val_loss: 0.6181 - 72s/epoch - 7s/step
Epoch 78/100
10/10 - 71s - loss: 0.5531 - val_loss: 0.5506 - 71s/epoch - 7s/step
Epoch 79/100
10/10 - 72s - loss: 0.5568 - val_loss: 0.5573 - 72s/epoch - 7s/step
Epoch 80/100
10/10 - 72s - loss: 0.5079 - val_loss: 0.5077 - 72s/epoch - 7s/step
Epoch 81/100
10/10 - 70s - loss: 0.5479 - val_loss: 0.5468 - 70s/epoch - 7s/step
Epoch 82/100
10/10 - 73s - loss: 0.6022 - val_loss: 0.6071 - 73s/epoch - 7s/step
Epoch 83/100
10/10 - 72s - loss: 0.4959 - val_loss: 0.4950 - 72s/epoch - 7s/step
Epoch 84/100
10/10 - 70s - loss: 0.4740 - val_loss: 0.4699 - 70s/epoch - 7s/step
Epoch 85/100
10/10 - 72s - loss: 0.5392 - val_loss: 0.5438 - 72s/epoch - 7s/step
Epoch 86/100
10/10 - 73s - loss: 0.5169 - val_loss: 0.5138 - 73s/epoch - 7s/step
Epoch 87/100
10/10 - 74s - loss: 0.5804 - val_loss: 0.5800 - 74s/epoch - 7s/step
Epoch 88/100
10/10 - 73s - loss: 0.6323 - val_loss: 0.6301 - 73s/epoch - 7s/step
Epoch 89/100
10/10 - 71s - loss: 0.5479 - val_loss: 0.5479 - 71s/epoch - 7s/step
Epoch 90/100
10/10 - 71s - loss: 0.5491 - val_loss: 0.5432 - 71s/epoch - 7s/step
Epoch 91/100
10/10 - 73s - loss: 0.5409 - val_loss: 0.5366 - 73s/epoch - 7s/step
Epoch 92/100
10/10 - 71s - loss: 0.5505 - val_loss: 0.5521 - 71s/epoch - 7s/step
Epoch 93/100
10/10 - 71s - loss: 0.5167 - val_loss: 0.5155 - 71s/epoch - 7s/step
Epoch 94/100
10/10 - 72s - loss: 0.5140 - val_loss: 0.5124 - 72s/epoch - 7s/step
Epoch 95/100
10/10 - 69s - loss: 0.4952 - val_loss: 0.5000 - 69s/epoch - 7s/step
Epoch 96/100
10/10 - 72s - loss: 0.5675 - val_loss: 0.5667 - 72s/epoch - 7s/step
Epoch 97/100
10/10 - 72s - loss: 0.5650 - val_loss: 0.5579 - 72s/epoch - 7s/step
Epoch 98/100
10/10 - 73s - loss: 0.5557 - val_loss: 0.5550 - 73s/epoch - 7s/step
Epoch 99/100
10/10 - 73s - loss: 0.5709 - val_loss: 0.5741 - 73s/epoch - 7s/step
Epoch 100/100
10/10 - 72s - loss: 0.5434 - val_loss: 0.5413 - 72s/epoch - 7s/step
