
The following modules were not unloaded:
   (Use "module --force purge" to unload all):

  1) XALT/minimal   2) slurm   3) NeSI
2024-04-24 02:06:45.092283: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-24 02:06:45.092380: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-24 02:06:45.092426: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-24 02:06:45.105245: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-24 02:06:49.595636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38298 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:83:00.0, compute capability: 8.0
1 Physical GPUs, 1 Logical GPUs
#################################
####DATA GENERATOR PARAMETERS####
#Dataset size:  11011
#Batch size:  8
#Time in years: 2.658143162020893
#n_channels:  2
#dt:  10
#Length of timeseries: 8388608
Noise background:  False
#################################
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d (Conv1D)             (None, 1048576, 64)       8256      
                                                                 
 conv1d_1 (Conv1D)           (None, 131072, 64)        262208    
                                                                 
 conv1d_2 (Conv1D)           (None, 16384, 64)         262208    
                                                                 
 conv1d_transpose (Conv1DTr  (None, 131072, 64)        262208    
 anspose)                                                        
                                                                 
 conv1d_transpose_1 (Conv1D  (None, 1048576, 64)       262208    
 Transpose)                                                      
                                                                 
 conv1d_transpose_2 (Conv1D  (None, 8388608, 2)        8194      
 Transpose)                                                      
                                                                 
 activation (Activation)     (None, 8388608, 2)        0         
                                                                 
=================================================================
Total params: 1065282 (4.06 MB)
Trainable params: 1065282 (4.06 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
Epoch 1/100
2024-04-24 02:07:25.611388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600
2024-04-24 02:08:55.862679: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ab41c327de0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-24 02:08:55.862779: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2024-04-24 02:08:55.875694: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-04-24 02:08:56.167617: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
10/10 - 169s - loss: 2.1154e-07 - val_loss: 2.0162e-07 - 169s/epoch - 17s/step
Epoch 2/100
10/10 - 73s - loss: 1.3695e-07 - val_loss: 1.3617e-07 - 73s/epoch - 7s/step
Epoch 3/100
10/10 - 74s - loss: 1.2248e-07 - val_loss: 1.2270e-07 - 74s/epoch - 7s/step
Epoch 4/100
10/10 - 73s - loss: 8.8471e-08 - val_loss: 8.6123e-08 - 73s/epoch - 7s/step
Epoch 5/100
10/10 - 72s - loss: 2.2958e-07 - val_loss: 2.3160e-07 - 72s/epoch - 7s/step
Epoch 6/100
10/10 - 74s - loss: 1.9201e-07 - val_loss: 1.9658e-07 - 74s/epoch - 7s/step
Epoch 7/100
10/10 - 72s - loss: 1.0088e-07 - val_loss: 1.0271e-07 - 72s/epoch - 7s/step
Epoch 8/100
10/10 - 72s - loss: 1.8824e-07 - val_loss: 1.8660e-07 - 72s/epoch - 7s/step
Epoch 9/100
10/10 - 71s - loss: 4.3383e-06 - val_loss: 2.6728e-07 - 71s/epoch - 7s/step
Epoch 10/100
10/10 - 71s - loss: 1.8898e-07 - val_loss: 1.8983e-07 - 71s/epoch - 7s/step
Epoch 11/100
10/10 - 70s - loss: 1.5250e-07 - val_loss: 1.3679e-07 - 70s/epoch - 7s/step
Epoch 12/100
10/10 - 74s - loss: 6.9313e-07 - val_loss: 6.1331e-07 - 74s/epoch - 7s/step
Epoch 13/100
10/10 - 72s - loss: 1.2859e-07 - val_loss: 1.4982e-07 - 72s/epoch - 7s/step
Epoch 14/100
10/10 - 74s - loss: 1.7720e-07 - val_loss: 2.0384e-07 - 74s/epoch - 7s/step
Epoch 15/100
10/10 - 74s - loss: 2.5570e-07 - val_loss: 2.6311e-07 - 74s/epoch - 7s/step
Epoch 16/100
10/10 - 77s - loss: 4.6390e-07 - val_loss: 7.7968e-07 - 77s/epoch - 8s/step
Epoch 17/100
10/10 - 81s - loss: 1.6701e-07 - val_loss: 1.6705e-07 - 81s/epoch - 8s/step
Epoch 18/100
10/10 - 85s - loss: 2.0026e-07 - val_loss: 1.9360e-07 - 85s/epoch - 9s/step
Epoch 19/100
10/10 - 84s - loss: 1.2332e-07 - val_loss: 1.2330e-07 - 84s/epoch - 8s/step
Epoch 20/100
10/10 - 85s - loss: 1.3850e-07 - val_loss: 1.5371e-07 - 85s/epoch - 8s/step
Epoch 21/100
10/10 - 80s - loss: 1.0156e-07 - val_loss: 1.0164e-07 - 80s/epoch - 8s/step
Epoch 22/100
10/10 - 80s - loss: 1.4171e-07 - val_loss: 1.4171e-07 - 80s/epoch - 8s/step
Epoch 23/100
10/10 - 79s - loss: 1.3739e-07 - val_loss: 2.1079e-07 - 79s/epoch - 8s/step
Epoch 24/100
10/10 - 80s - loss: 1.8212e-07 - val_loss: 2.3594e-07 - 80s/epoch - 8s/step
Epoch 25/100
10/10 - 79s - loss: 1.5191e-07 - val_loss: 1.3497e-07 - 79s/epoch - 8s/step
Epoch 26/100
10/10 - 78s - loss: 1.7201e-07 - val_loss: 1.8808e-07 - 78s/epoch - 8s/step
Epoch 27/100
10/10 - 76s - loss: 3.9352e-07 - val_loss: 1.8871e-07 - 76s/epoch - 8s/step
Epoch 28/100
10/10 - 78s - loss: 1.4000e-07 - val_loss: 1.4384e-07 - 78s/epoch - 8s/step
Epoch 29/100
10/10 - 73s - loss: 1.9822e-07 - val_loss: 1.9607e-07 - 73s/epoch - 7s/step
Epoch 30/100
10/10 - 77s - loss: 2.2769e-07 - val_loss: 1.9454e-07 - 77s/epoch - 8s/step
Epoch 31/100
10/10 - 73s - loss: 1.1060e-06 - val_loss: 3.3203e-06 - 73s/epoch - 7s/step
Epoch 32/100
10/10 - 73s - loss: 1.7477e-07 - val_loss: 1.5959e-07 - 73s/epoch - 7s/step
Epoch 33/100
10/10 - 72s - loss: 1.6058e-07 - val_loss: 1.5341e-07 - 72s/epoch - 7s/step
Epoch 34/100
10/10 - 71s - loss: 1.5027e-07 - val_loss: 1.5617e-07 - 71s/epoch - 7s/step
Epoch 35/100
10/10 - 73s - loss: 6.6946e-07 - val_loss: 3.8107e-07 - 73s/epoch - 7s/step
Epoch 36/100
10/10 - 71s - loss: 1.8449e-07 - val_loss: 1.6163e-07 - 71s/epoch - 7s/step
Epoch 37/100
10/10 - 73s - loss: 1.7021e-07 - val_loss: 1.6108e-07 - 73s/epoch - 7s/step
Epoch 38/100
10/10 - 73s - loss: 5.5610e-07 - val_loss: 5.8755e-07 - 73s/epoch - 7s/step
Epoch 39/100
10/10 - 70s - loss: 3.5022e-07 - val_loss: 2.1052e-07 - 70s/epoch - 7s/step
Epoch 40/100
10/10 - 70s - loss: 2.3159e-07 - val_loss: 1.8726e-07 - 70s/epoch - 7s/step
Epoch 41/100
10/10 - 74s - loss: 2.5368e-07 - val_loss: 2.3272e-07 - 74s/epoch - 7s/step
Epoch 42/100
10/10 - 76s - loss: 1.7582e-07 - val_loss: 1.7688e-07 - 76s/epoch - 8s/step
Epoch 43/100
10/10 - 90s - loss: 4.1549e-07 - val_loss: 2.0065e-07 - 90s/epoch - 9s/step
Epoch 44/100
10/10 - 90s - loss: 1.8620e-07 - val_loss: 2.4768e-07 - 90s/epoch - 9s/step
Epoch 45/100
10/10 - 90s - loss: 3.5077e-07 - val_loss: 1.6663e-07 - 90s/epoch - 9s/step
Epoch 46/100
10/10 - 90s - loss: 1.4649e-07 - val_loss: 1.4796e-07 - 90s/epoch - 9s/step
Epoch 47/100
10/10 - 90s - loss: 1.6712e-07 - val_loss: 1.9191e-07 - 90s/epoch - 9s/step
Epoch 48/100
10/10 - 90s - loss: 1.8243e-07 - val_loss: 1.4831e-07 - 90s/epoch - 9s/step
Epoch 49/100
10/10 - 77s - loss: 1.3943e-07 - val_loss: 1.6637e-07 - 77s/epoch - 8s/step
Epoch 50/100
10/10 - 74s - loss: 2.2241e-07 - val_loss: 2.0541e-07 - 74s/epoch - 7s/step
Epoch 51/100
10/10 - 79s - loss: 2.0253e-07 - val_loss: 1.9242e-07 - 79s/epoch - 8s/step
Epoch 52/100
10/10 - 83s - loss: 1.6895e-07 - val_loss: 1.5226e-07 - 83s/epoch - 8s/step
Epoch 53/100
10/10 - 80s - loss: 1.5235e-07 - val_loss: 1.4986e-07 - 80s/epoch - 8s/step
Epoch 54/100
10/10 - 74s - loss: 1.9080e-07 - val_loss: 2.1302e-07 - 74s/epoch - 7s/step
Epoch 55/100
10/10 - 74s - loss: 1.7107e-07 - val_loss: 1.2505e-07 - 74s/epoch - 7s/step
Epoch 56/100
10/10 - 72s - loss: 1.3201e-07 - val_loss: 1.3641e-07 - 72s/epoch - 7s/step
Epoch 57/100
10/10 - 79s - loss: 2.0296e-07 - val_loss: 2.4049e-07 - 79s/epoch - 8s/step
Epoch 58/100
10/10 - 75s - loss: 1.8128e-07 - val_loss: 2.1730e-07 - 75s/epoch - 7s/step
Epoch 59/100
10/10 - 78s - loss: 2.1250e-07 - val_loss: 1.8132e-07 - 78s/epoch - 8s/step
Epoch 60/100
10/10 - 73s - loss: 1.3358e-07 - val_loss: 3.0249e-07 - 73s/epoch - 7s/step
Epoch 61/100
10/10 - 74s - loss: 1.5888e-07 - val_loss: 1.6842e-07 - 74s/epoch - 7s/step
Epoch 62/100
10/10 - 79s - loss: 2.4553e-07 - val_loss: 1.6095e-07 - 79s/epoch - 8s/step
Epoch 63/100
10/10 - 83s - loss: 1.6652e-07 - val_loss: 2.2289e-07 - 83s/epoch - 8s/step
Epoch 64/100
10/10 - 81s - loss: 5.4110e-07 - val_loss: 3.5356e-07 - 81s/epoch - 8s/step
Epoch 65/100
10/10 - 78s - loss: 1.4213e-07 - val_loss: 1.2716e-07 - 78s/epoch - 8s/step
Epoch 66/100
10/10 - 84s - loss: 1.9116e-07 - val_loss: 1.9116e-07 - 84s/epoch - 8s/step
Epoch 67/100
10/10 - 86s - loss: 1.5572e-07 - val_loss: 1.5004e-07 - 86s/epoch - 9s/step
Epoch 68/100
10/10 - 88s - loss: 1.0818e-07 - val_loss: 1.0877e-07 - 88s/epoch - 9s/step
Epoch 69/100
10/10 - 87s - loss: 1.5730e-07 - val_loss: 1.5435e-07 - 87s/epoch - 9s/step
Epoch 70/100
10/10 - 86s - loss: 1.4013e-07 - val_loss: 1.4016e-07 - 86s/epoch - 9s/step
Epoch 71/100
10/10 - 87s - loss: 1.2720e-07 - val_loss: 1.2710e-07 - 87s/epoch - 9s/step
Epoch 72/100
10/10 - 86s - loss: 1.0763e-07 - val_loss: 1.0532e-07 - 86s/epoch - 9s/step
Epoch 73/100
10/10 - 86s - loss: 1.3189e-07 - val_loss: 1.2387e-07 - 86s/epoch - 9s/step
Epoch 74/100
10/10 - 90s - loss: 1.7509e-07 - val_loss: 1.7522e-07 - 90s/epoch - 9s/step
Epoch 75/100
10/10 - 87s - loss: 2.5025e-07 - val_loss: 2.5024e-07 - 87s/epoch - 9s/step
Epoch 76/100
10/10 - 86s - loss: 1.5326e-07 - val_loss: 1.6382e-07 - 86s/epoch - 9s/step
Epoch 77/100
10/10 - 90s - loss: 2.2592e-07 - val_loss: 2.3520e-07 - 90s/epoch - 9s/step
Epoch 78/100
10/10 - 89s - loss: 1.3442e-07 - val_loss: 1.3450e-07 - 89s/epoch - 9s/step
Epoch 79/100
10/10 - 89s - loss: 1.5170e-07 - val_loss: 1.1743e-07 - 89s/epoch - 9s/step
Epoch 80/100
10/10 - 88s - loss: 1.1641e-07 - val_loss: 1.1641e-07 - 88s/epoch - 9s/step
Epoch 81/100
10/10 - 86s - loss: 1.2081e-07 - val_loss: 1.1671e-07 - 86s/epoch - 9s/step
Epoch 82/100
10/10 - 89s - loss: 2.0495e-07 - val_loss: 2.0470e-07 - 89s/epoch - 9s/step
Epoch 83/100
10/10 - 88s - loss: 1.4763e-07 - val_loss: 7.4614e-08 - 88s/epoch - 9s/step
Epoch 84/100
10/10 - 85s - loss: 1.5797e-07 - val_loss: 1.5243e-07 - 85s/epoch - 9s/step
Epoch 85/100
10/10 - 86s - loss: 1.5068e-07 - val_loss: 1.2914e-07 - 86s/epoch - 9s/step
Epoch 86/100
10/10 - 92s - loss: 1.3019e-07 - val_loss: 1.3017e-07 - 92s/epoch - 9s/step
Epoch 87/100
10/10 - 92s - loss: 2.0566e-07 - val_loss: 2.1015e-07 - 92s/epoch - 9s/step
Epoch 88/100
10/10 - 90s - loss: 2.5687e-07 - val_loss: 2.5345e-07 - 90s/epoch - 9s/step
Epoch 89/100
10/10 - 89s - loss: 1.0885e-07 - val_loss: 1.0884e-07 - 89s/epoch - 9s/step
Epoch 90/100
10/10 - 87s - loss: 1.7345e-07 - val_loss: 2.0163e-07 - 87s/epoch - 9s/step
Epoch 91/100
10/10 - 88s - loss: 1.8454e-07 - val_loss: 1.8452e-07 - 88s/epoch - 9s/step
Epoch 92/100
10/10 - 86s - loss: 1.3678e-07 - val_loss: 1.5105e-07 - 86s/epoch - 9s/step
Epoch 93/100
10/10 - 88s - loss: 1.5369e-07 - val_loss: 1.6679e-07 - 88s/epoch - 9s/step
Epoch 94/100
10/10 - 89s - loss: 1.4622e-07 - val_loss: 1.4616e-07 - 89s/epoch - 9s/step
Epoch 95/100
10/10 - 86s - loss: 9.7037e-08 - val_loss: 9.7036e-08 - 86s/epoch - 9s/step
Epoch 96/100
10/10 - 87s - loss: 1.7053e-07 - val_loss: 1.7052e-07 - 87s/epoch - 9s/step
Epoch 97/100
10/10 - 87s - loss: 2.7923e-07 - val_loss: 2.7924e-07 - 87s/epoch - 9s/step
Epoch 98/100
10/10 - 87s - loss: 1.3081e-07 - val_loss: 1.3081e-07 - 87s/epoch - 9s/step
Epoch 99/100
10/10 - 85s - loss: 2.1532e-07 - val_loss: 2.1534e-07 - 85s/epoch - 9s/step
Epoch 100/100
10/10 - 85s - loss: 1.4274e-07 - val_loss: 1.4430e-07 - 85s/epoch - 9s/step
