
The following modules were not unloaded:
   (Use "module --force purge" to unload all):

  1) XALT/minimal   2) slurm   3) NeSI
2024-03-19 00:30:57.513061: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-03-19 00:30:57.513139: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-03-19 00:30:57.513175: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-03-19 00:30:57.524130: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-03-19 00:31:01.564935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38298 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:05:00.0, compute capability: 8.0
1 Physical GPUs, 1 Logical GPUs
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d (Conv1D)             (None, 1048576, 8)        2056      
                                                                 
 conv1d_1 (Conv1D)           (None, 262144, 8)         8200      
                                                                 
 conv1d_transpose (Conv1DTr  (None, 1048576, 8)        8200      
 anspose)                                                        
                                                                 
 conv1d_transpose_1 (Conv1D  (None, 4194304, 8)        8200      
 Transpose)                                                      
                                                                 
 conv1d_transpose_2 (Conv1D  (None, 4194304, 2)        18        
 Transpose)                                                      
                                                                 
 activation (Activation)     (None, 4194304, 2)        0         
                                                                 
=================================================================
Total params: 26674 (104.20 KB)
Trainable params: 26674 (104.20 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
#################################
####DATA GENERATOR PARAMETERS####
#Batch size:  8
#Time in years: 1.3290715810104465
#n_channels:  2
#dt:  10
#Length of timeseries: 4194304
#################################
Epoch 1/150
2024-03-19 00:31:22.693058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600
2024-03-19 00:32:07.410401: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2aac180135a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-03-19 00:32:07.411740: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2024-03-19 00:32:07.419896: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-03-19 00:32:07.659918: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
1/1 [==============================] - ETA: 0s - loss: 0.92741/1 [==============================] - 1s 598ms/step - loss: 0.9274
Noise loss:  0.9274137616157532
1/1 - 57s - loss: 0.9365 - val_loss: 0.9269 - 57s/epoch - 57s/step
Epoch 2/150
1/1 [==============================] - ETA: 0s - loss: 0.91861/1 [==============================] - 0s 416ms/step - loss: 0.9186
Noise loss:  0.9185718297958374
1/1 - 7s - loss: 0.9270 - val_loss: 0.9189 - 7s/epoch - 7s/step
Epoch 3/150
1/1 [==============================] - ETA: 0s - loss: 0.91081/1 [==============================] - 0s 399ms/step - loss: 0.9108
Noise loss:  0.910756528377533
1/1 - 7s - loss: 0.9186 - val_loss: 0.9104 - 7s/epoch - 7s/step
Epoch 4/150
1/1 [==============================] - ETA: 0s - loss: 0.90341/1 [==============================] - 0s 395ms/step - loss: 0.9034
Noise loss:  0.9033874273300171
1/1 - 7s - loss: 0.9109 - val_loss: 0.9035 - 7s/epoch - 7s/step
Epoch 5/150
1/1 [==============================] - ETA: 0s - loss: 0.89651/1 [==============================] - 0s 418ms/step - loss: 0.8965
Noise loss:  0.896453857421875
1/1 - 7s - loss: 0.9032 - val_loss: 0.8966 - 7s/epoch - 7s/step
Epoch 6/150
1/1 [==============================] - ETA: 0s - loss: 0.88931/1 [==============================] - 0s 401ms/step - loss: 0.8893
Noise loss:  0.8892912268638611
1/1 - 7s - loss: 0.8964 - val_loss: 0.8895 - 7s/epoch - 7s/step
Epoch 7/150
1/1 [==============================] - ETA: 0s - loss: 0.88301/1 [==============================] - 0s 431ms/step - loss: 0.8830
Noise loss:  0.8829931616783142
1/1 - 7s - loss: 0.8898 - val_loss: 0.8825 - 7s/epoch - 7s/step
Epoch 8/150
