
The following modules were not unloaded:
   (Use "module --force purge" to unload all):

  1) XALT/minimal   2) slurm   3) NeSI
2024-04-18 14:55:30.108404: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-04-18 14:55:30.108503: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-04-18 14:55:30.108545: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-04-18 14:55:30.120260: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-18 14:55:34.175891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38298 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:83:00.0, compute capability: 8.0
1 Physical GPUs, 1 Logical GPUs
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv1d (Conv1D)             (None, 1048576, 64)       8256      
                                                                 
 conv1d_1 (Conv1D)           (None, 131072, 64)        262208    
                                                                 
 conv1d_2 (Conv1D)           (None, 16384, 64)         262208    
                                                                 
 conv1d_transpose (Conv1DTr  (None, 131072, 64)        262208    
 anspose)                                                        
                                                                 
 conv1d_transpose_1 (Conv1D  (None, 1048576, 64)       262208    
 Transpose)                                                      
                                                                 
 conv1d_transpose_2 (Conv1D  (None, 8388608, 2)        8194      
 Transpose)                                                      
                                                                 
 activation (Activation)     (None, 8388608, 2)        0         
                                                                 
=================================================================
Total params: 1065282 (4.06 MB)
Trainable params: 1065282 (4.06 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
#################################
####DATA GENERATOR PARAMETERS####
#Dataset size:  11011
#Batch size:  8
#Time in years: 2.658143162020893
#n_channels:  2
#dt:  10
#Length of timeseries: 8388608
Noise background:  False
#################################
Epoch 1/100
2024-04-18 14:56:08.058811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600
2024-04-18 14:57:23.913695: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2ab5edce7830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-18 14:57:23.913763: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2024-04-18 14:57:23.952249: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2024-04-18 14:57:24.538165: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
10/10 - 153s - loss: 0.8118 - val_loss: 0.9239 - 153s/epoch - 15s/step
Epoch 2/100
10/10 - 68s - loss: 0.7516 - val_loss: 0.9599 - 68s/epoch - 7s/step
Epoch 3/100
10/10 - 69s - loss: 0.8859 - val_loss: 0.8757 - 69s/epoch - 7s/step
Epoch 4/100
10/10 - 68s - loss: 0.8077 - val_loss: 0.8044 - 68s/epoch - 7s/step
Epoch 5/100
10/10 - 68s - loss: 0.7882 - val_loss: 0.7107 - 68s/epoch - 7s/step
Epoch 6/100
10/10 - 70s - loss: 0.7580 - val_loss: 0.7086 - 70s/epoch - 7s/step
Epoch 7/100
10/10 - 68s - loss: 0.8380 - val_loss: 0.7070 - 68s/epoch - 7s/step
Epoch 8/100
10/10 - 68s - loss: 0.8793 - val_loss: 0.8968 - 68s/epoch - 7s/step
Epoch 9/100
10/10 - 68s - loss: 0.9230 - val_loss: 0.9209 - 68s/epoch - 7s/step
Epoch 10/100
10/10 - 68s - loss: 0.8395 - val_loss: 0.7921 - 68s/epoch - 7s/step
Epoch 11/100
10/10 - 67s - loss: 0.8888 - val_loss: 0.7993 - 67s/epoch - 7s/step
Epoch 12/100
10/10 - 70s - loss: 0.7583 - val_loss: 0.9749 - 70s/epoch - 7s/step
Epoch 13/100
10/10 - 68s - loss: 0.8867 - val_loss: 0.7558 - 68s/epoch - 7s/step
Epoch 14/100
10/10 - 70s - loss: 0.8323 - val_loss: 0.6533 - 70s/epoch - 7s/step
Epoch 15/100
10/10 - 70s - loss: 0.7892 - val_loss: 0.8297 - 70s/epoch - 7s/step
Epoch 16/100
10/10 - 70s - loss: 0.8655 - val_loss: 0.8211 - 70s/epoch - 7s/step
Epoch 17/100
10/10 - 67s - loss: 0.7461 - val_loss: 0.6234 - 67s/epoch - 7s/step
Epoch 18/100
10/10 - 69s - loss: 0.9647 - val_loss: 0.8308 - 69s/epoch - 7s/step
Epoch 19/100
10/10 - 69s - loss: 0.8395 - val_loss: 0.8374 - 69s/epoch - 7s/step
Epoch 20/100
10/10 - 70s - loss: 0.9158 - val_loss: 0.7952 - 70s/epoch - 7s/step
Epoch 21/100
10/10 - 67s - loss: 0.9569 - val_loss: 0.7489 - 67s/epoch - 7s/step
Epoch 22/100
10/10 - 68s - loss: 0.8268 - val_loss: 0.9100 - 68s/epoch - 7s/step
Epoch 23/100
10/10 - 67s - loss: 0.7679 - val_loss: 0.7711 - 67s/epoch - 7s/step
Epoch 24/100
10/10 - 69s - loss: 0.8867 - val_loss: 0.7064 - 69s/epoch - 7s/step
Epoch 25/100
10/10 - 67s - loss: 0.7873 - val_loss: 0.7350 - 67s/epoch - 7s/step
Epoch 26/100
10/10 - 68s - loss: 0.7078 - val_loss: 0.8267 - 68s/epoch - 7s/step
Epoch 27/100
10/10 - 69s - loss: 0.8215 - val_loss: 0.7384 - 69s/epoch - 7s/step
Epoch 28/100
10/10 - 70s - loss: 0.6825 - val_loss: 0.7625 - 70s/epoch - 7s/step
Epoch 29/100
10/10 - 70s - loss: 0.6270 - val_loss: 0.7039 - 70s/epoch - 7s/step
Epoch 30/100
10/10 - 68s - loss: 0.7507 - val_loss: 0.7192 - 68s/epoch - 7s/step
Epoch 31/100
10/10 - 69s - loss: 0.8280 - val_loss: 0.6826 - 69s/epoch - 7s/step
Epoch 32/100
10/10 - 69s - loss: 0.5858 - val_loss: 0.6842 - 69s/epoch - 7s/step
Epoch 33/100
10/10 - 68s - loss: 0.7518 - val_loss: 0.7042 - 68s/epoch - 7s/step
Epoch 34/100
10/10 - 67s - loss: 0.6056 - val_loss: 0.7063 - 67s/epoch - 7s/step
Epoch 35/100
10/10 - 70s - loss: 0.6597 - val_loss: 0.7703 - 70s/epoch - 7s/step
Epoch 36/100
10/10 - 67s - loss: 0.7120 - val_loss: 0.7368 - 67s/epoch - 7s/step
Epoch 37/100
10/10 - 69s - loss: 0.5845 - val_loss: 0.7179 - 69s/epoch - 7s/step
Epoch 38/100
10/10 - 69s - loss: 0.6813 - val_loss: 0.6080 - 69s/epoch - 7s/step
Epoch 39/100
10/10 - 66s - loss: 0.6438 - val_loss: 0.6752 - 66s/epoch - 7s/step
Epoch 40/100
10/10 - 67s - loss: 0.6532 - val_loss: 0.5611 - 67s/epoch - 7s/step
Epoch 41/100
10/10 - 71s - loss: 0.6232 - val_loss: 0.7409 - 71s/epoch - 7s/step
Epoch 42/100
10/10 - 69s - loss: 0.6288 - val_loss: 0.7612 - 69s/epoch - 7s/step
Epoch 43/100
10/10 - 67s - loss: 0.5432 - val_loss: 0.4673 - 67s/epoch - 7s/step
Epoch 44/100
10/10 - 69s - loss: 0.5940 - val_loss: 0.6471 - 69s/epoch - 7s/step
Epoch 45/100
10/10 - 76s - loss: 0.6779 - val_loss: 0.7300 - 76s/epoch - 8s/step
Epoch 46/100
10/10 - 68s - loss: 0.5351 - val_loss: 0.6680 - 68s/epoch - 7s/step
Epoch 47/100
10/10 - 68s - loss: 0.7123 - val_loss: 0.7759 - 68s/epoch - 7s/step
Epoch 48/100
10/10 - 68s - loss: 0.5799 - val_loss: 0.7078 - 68s/epoch - 7s/step
Epoch 49/100
10/10 - 67s - loss: 0.6630 - val_loss: 0.6486 - 67s/epoch - 7s/step
Epoch 50/100
10/10 - 66s - loss: 0.7042 - val_loss: 0.5008 - 66s/epoch - 7s/step
Epoch 51/100
10/10 - 68s - loss: 0.6022 - val_loss: 0.7012 - 68s/epoch - 7s/step
Epoch 52/100
10/10 - 69s - loss: 0.6412 - val_loss: 0.7290 - 69s/epoch - 7s/step
Epoch 53/100
10/10 - 65s - loss: 0.5885 - val_loss: 0.5523 - 65s/epoch - 7s/step
Epoch 54/100
10/10 - 65s - loss: 0.6289 - val_loss: 0.5947 - 65s/epoch - 6s/step
Epoch 55/100
10/10 - 68s - loss: 0.6796 - val_loss: 0.7097 - 68s/epoch - 7s/step
Epoch 56/100
10/10 - 65s - loss: 0.6540 - val_loss: 0.6000 - 65s/epoch - 7s/step
Epoch 57/100
10/10 - 68s - loss: 0.6638 - val_loss: 0.5892 - 68s/epoch - 7s/step
Epoch 58/100
10/10 - 68s - loss: 0.6583 - val_loss: 0.5812 - 68s/epoch - 7s/step
Epoch 59/100
10/10 - 68s - loss: 0.5373 - val_loss: 0.6189 - 68s/epoch - 7s/step
Epoch 60/100
10/10 - 67s - loss: 0.6150 - val_loss: 0.5762 - 67s/epoch - 7s/step
Epoch 61/100
10/10 - 67s - loss: 0.6033 - val_loss: 0.5251 - 67s/epoch - 7s/step
Epoch 62/100
10/10 - 67s - loss: 0.7141 - val_loss: 0.4888 - 67s/epoch - 7s/step
Epoch 63/100
10/10 - 70s - loss: 0.7291 - val_loss: 0.4821 - 70s/epoch - 7s/step
Epoch 64/100
10/10 - 68s - loss: 0.4704 - val_loss: 0.5943 - 68s/epoch - 7s/step
Epoch 65/100
10/10 - 67s - loss: 0.5860 - val_loss: 0.5975 - 67s/epoch - 7s/step
Epoch 66/100
10/10 - 67s - loss: 0.4269 - val_loss: 0.5143 - 67s/epoch - 7s/step
Epoch 67/100
10/10 - 66s - loss: 0.6005 - val_loss: 0.5622 - 66s/epoch - 7s/step
Epoch 68/100
10/10 - 68s - loss: 0.5652 - val_loss: 0.5574 - 68s/epoch - 7s/step
Epoch 69/100
10/10 - 68s - loss: 0.5155 - val_loss: 0.5823 - 68s/epoch - 7s/step
Epoch 70/100
10/10 - 67s - loss: 0.4856 - val_loss: 0.7591 - 67s/epoch - 7s/step
Epoch 71/100
10/10 - 68s - loss: 0.6307 - val_loss: 0.5637 - 68s/epoch - 7s/step
Epoch 72/100
10/10 - 67s - loss: 0.5702 - val_loss: 0.7038 - 67s/epoch - 7s/step
Epoch 73/100
10/10 - 66s - loss: 0.5049 - val_loss: 0.5648 - 66s/epoch - 7s/step
Epoch 74/100
10/10 - 69s - loss: 0.6060 - val_loss: 0.6327 - 69s/epoch - 7s/step
Epoch 75/100
10/10 - 67s - loss: 0.6638 - val_loss: 0.6120 - 67s/epoch - 7s/step
Epoch 76/100
10/10 - 67s - loss: 0.6666 - val_loss: 0.5181 - 67s/epoch - 7s/step
Epoch 77/100
10/10 - 68s - loss: 0.6124 - val_loss: 0.6083 - 68s/epoch - 7s/step
Epoch 78/100
10/10 - 67s - loss: 0.4741 - val_loss: 0.5831 - 67s/epoch - 7s/step
Epoch 79/100
10/10 - 68s - loss: 0.5646 - val_loss: 0.6766 - 68s/epoch - 7s/step
Epoch 80/100
10/10 - 67s - loss: 0.5198 - val_loss: 0.5372 - 67s/epoch - 7s/step
Epoch 81/100
10/10 - 65s - loss: 0.6850 - val_loss: 0.5494 - 65s/epoch - 7s/step
Epoch 82/100
10/10 - 68s - loss: 0.6190 - val_loss: 0.7052 - 68s/epoch - 7s/step
Epoch 83/100
10/10 - 67s - loss: 0.6156 - val_loss: 0.5335 - 67s/epoch - 7s/step
Epoch 84/100
10/10 - 66s - loss: 0.5475 - val_loss: 0.4475 - 66s/epoch - 7s/step
Epoch 85/100
10/10 - 68s - loss: 0.6178 - val_loss: 0.4978 - 68s/epoch - 7s/step
Epoch 86/100
10/10 - 68s - loss: 0.5597 - val_loss: 0.5869 - 68s/epoch - 7s/step
Epoch 87/100
10/10 - 69s - loss: 0.6453 - val_loss: 0.5480 - 69s/epoch - 7s/step
Epoch 88/100
10/10 - 69s - loss: 0.5754 - val_loss: 0.5973 - 69s/epoch - 7s/step
Epoch 89/100
10/10 - 67s - loss: 0.6061 - val_loss: 0.5373 - 67s/epoch - 7s/step
Epoch 90/100
10/10 - 66s - loss: 0.5467 - val_loss: 0.6375 - 66s/epoch - 7s/step
Epoch 91/100
10/10 - 68s - loss: 0.5620 - val_loss: 0.6672 - 68s/epoch - 7s/step
Epoch 92/100
10/10 - 75s - loss: 0.5337 - val_loss: 0.5587 - 75s/epoch - 7s/step
Epoch 93/100
10/10 - 67s - loss: 0.3953 - val_loss: 0.5557 - 67s/epoch - 7s/step
slurmstepd: error: *** JOB 45701606 ON wbl004 CANCELLED AT 2024-04-18T04:55:38 DUE TO TIME LIMIT ***
